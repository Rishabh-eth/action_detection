
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


[11/25 22:49:55][INFO] train_net.py: 405: Train with config:
[11/25 22:49:55][INFO] train_net.py: 406: {'AVA': {'ANNOTATION_DIR': '/srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/',
         'BGR': False,
         'DETECTION_SCORE_THRESH': 0.8,
         'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv',
         'FRAME_DIR': '/srv/beegfs02/scratch/da_action/data/ava/frames/',
         'FRAME_LIST_DIR': '/srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/',
         'FULL_TEST_ON_VAL': True,
         'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv',
         'IMG_PROC_BACKEND': 'cv2',
         'LABEL_MAP_FILE': 'ava_action_list_v2.2.pbtxt',
         'TEST_FORCE_FLIP': False,
         'TEST_LISTS': ['val.csv'],
         'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'],
         'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'],
         'TRAIN_LISTS': ['train.csv'],
         'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229],
         'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009],
                              [-0.5808, -0.0045, -0.814],
                              [-0.5836, -0.6948, 0.4203]],
         'TRAIN_PCA_JITTER_ONLY': True,
         'TRAIN_PREDICT_BOX_LISTS': [],
         'TRAIN_USE_COLOR_AUGMENTATION': False},
 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}),
 'BN': {'NORM_TYPE': 'batchnorm',
        'NUM_BATCHES_PRECISE': 200,
        'NUM_SPLITS': 1,
        'NUM_SYNC_DEVICES': 1,
        'USE_PRECISE_STATS': False,
        'WEIGHT_DECAY': 0.0},
 'DATA': {'DECODING_BACKEND': 'pyav',
          'ENSEMBLE_METHOD': 'sum',
          'INPUT_CHANNEL_NUM': [3, 3],
          'INV_UNIFORM_SAMPLE': False,
          'MEAN': [0.45, 0.45, 0.45],
          'MULTI_LABEL': False,
          'NUM_FRAMES': 32,
          'PATH_LABEL_SEPARATOR': ' ',
          'PATH_PREFIX': '',
          'PATH_TO_DATA_DIR': '',
          'RANDOM_FLIP': True,
          'REVERSE_INPUT_CHANNEL': False,
          'SAMPLING_RATE': 2,
          'STD': [0.225, 0.225, 0.225],
          'TARGET_FPS': 30,
          'TEST_CROP_SIZE': 256,
          'TRAIN_CROP_SIZE': 224,
          'TRAIN_JITTER_SCALES': [256, 320]},
 'DATA_LOADER': {'ENABLE_MULTI_THREAD_DECODE': False,
                 'NUM_WORKERS': 1,
                 'PIN_MEMORY': True},
 'DEMO': {'BUFFER_SIZE': 0,
          'CLIP_VIS_SIZE': 10,
          'COMMON_CLASS_NAMES': ['watch (a person)',
                                 'talk to (e.g., self, a person, a group)',
                                 'listen to (a person)',
                                 'touch (an object)',
                                 'carry/hold (an object)',
                                 'walk',
                                 'sit',
                                 'lie/sleep',
                                 'bend/bow (at the waist)'],
          'COMMON_CLASS_THRES': 0.7,
          'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',
          'DETECTRON2_THRESH': 0.9,
          'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',
          'DISPLAY_HEIGHT': 0,
          'DISPLAY_WIDTH': 0,
          'ENABLE': False,
          'FPS': 30,
          'GT_BOXES': '',
          'INPUT_FORMAT': 'BGR',
          'INPUT_VIDEO': '',
          'LABEL_FILE_PATH': '',
          'NUM_CLIPS_SKIP': 0,
          'NUM_VIS_INSTANCES': 2,
          'OUTPUT_FILE': '',
          'OUTPUT_FPS': -1,
          'PREDS_BOXES': '',
          'SLOWMO': 1,
          'STARTING_SECOND': 900,
          'THREAD_ENABLE': False,
          'UNCOMMON_CLASS_THRES': 0.3,
          'VIS_MODE': 'thres',
          'WEBCAM': -1},
 'DETECTION': {'ALIGNED': False,
               'ENABLE': True,
               'ROI_XFORM_RESOLUTION': 7,
               'SPATIAL_SCALE_FACTOR': 16},
 'DIST_BACKEND': 'nccl',
 'LOG_MODEL_INFO': False,
 'LOG_PERIOD': 10,
 'MODEL': {'ARCH': 'slowfast',
           'DROPCONNECT_RATE': 0.0,
           'DROPOUT_RATE': 0.5,
           'FC_INIT_STD': 0.01,
           'FREEZE_TO': 605,
           'HEAD_ACT': 'sigmoid',
           'LOSS_FUNC': 'bce',
           'MODEL_NAME': 'SlowFast',
           'MULTI_PATHWAY_ARCH': ['slowfast'],
           'NUM_CLASSES': 10,
           'SINGLE_PATHWAY_ARCH': ['c2d', 'i3d', 'slow', 'x3d']},
 'MULTIGRID': {'BN_BASE_SIZE': 8,
               'DEFAULT_B': 0,
               'DEFAULT_S': 0,
               'DEFAULT_T': 0,
               'EPOCH_FACTOR': 1.5,
               'EVAL_FREQ': 3,
               'LONG_CYCLE': False,
               'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476),
                                      (0.5, 0.7071067811865476),
                                      (0.5, 1),
                                      (1, 1)],
               'LONG_CYCLE_SAMPLING_RATE': 0,
               'SHORT_CYCLE': False,
               'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476]},
 'NONLOCAL': {'GROUP': [[1, 1], [1, 1], [1, 1], [1, 1]],
              'INSTANTIATION': 'dot_product',
              'LOCATION': [[[], []], [[], []], [[6, 13, 20], []], [[], []]],
              'POOL': [[[2, 2, 2], [2, 2, 2]],
                       [[2, 2, 2], [2, 2, 2]],
                       [[2, 2, 2], [2, 2, 2]],
                       [[2, 2, 2], [2, 2, 2]]]},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': '/srv/beegfs02/scratch/da_action/data/output/ex_10_500_100_v4',
 'RESNET': {'DEPTH': 101,
            'INPLACE_RELU': True,
            'NUM_BLOCK_TEMP_KERNEL': [[3, 3], [4, 4], [6, 6], [3, 3]],
            'NUM_GROUPS': 1,
            'SPATIAL_DILATIONS': [[1, 1], [1, 1], [1, 1], [2, 2]],
            'SPATIAL_STRIDES': [[1, 1], [2, 2], [2, 2], [1, 1]],
            'STRIDE_1X1': False,
            'TRANS_FUNC': 'bottleneck_transform',
            'WIDTH_PER_GROUP': 64,
            'ZERO_INIT_FINAL_BN': True},
 'RNG_SEED': 0,
 'SHARD_ID': 0,
 'SLOWFAST': {'ALPHA': 4,
              'BETA_INV': 8,
              'FUSION_CONV_CHANNEL_RATIO': 2,
              'FUSION_KERNEL_SZ': 5},
 'SOLVER': {'BASE_LR': 0.1,
            'BASE_LR_SCALE_NUM_SHARDS': False,
            'COSINE_END_LR': 0.0,
            'DAMPENING': 0.0,
            'GAMMA': 0.1,
            'LRS': [],
            'LR_POLICY': 'cosine',
            'MAX_EPOCH': 40,
            'MOMENTUM': 0.9,
            'NESTEROV': True,
            'OPTIMIZING_METHOD': 'sgd',
            'STEPS': [],
            'STEP_SIZE': 1,
            'WARMUP_EPOCHS': 0.0,
            'WARMUP_FACTOR': 0.1,
            'WARMUP_START_LR': 0.01,
            'WEIGHT_DECAY': 1e-07},
 'TENSORBOARD': {'CATEGORIES_PATH': '',
                 'CLASS_NAMES_PATH': '',
                 'CONFUSION_MATRIX': {'ENABLE': False,
                                      'FIGSIZE': [8, 8],
                                      'SUBSET_PATH': ''},
                 'ENABLE': True,
                 'HISTOGRAM': {'ENABLE': False,
                               'FIGSIZE': [8, 8],
                               'SUBSET_PATH': '',
                               'TOPK': 10},
                 'LOG_DIR': 'tensorboard',
                 'MODEL_VIS': {'ACTIVATIONS': False,
                               'COLORMAP': 'Pastel2',
                               'ENABLE': False,
                               'GRAD_CAM': {'COLORMAP': 'viridis',
                                            'ENABLE': True,
                                            'LAYER_LIST': [],
                                            'USE_TRUE_LABEL': False},
                               'INPUT_VIDEO': False,
                               'LAYER_LIST': [],
                               'MODEL_WEIGHTS': False,
                               'TOPK_PREDS': 1},
                 'PREDICTIONS_PATH': '',
                 'WRONG_PRED_VIS': {'ENABLE': False,
                                    'SUBSET_PATH': '',
                                    'TAG': 'Incorrectly classified videos.'}},
 'TEST': {'BATCH_SIZE': 4,
          'CHECKPOINT_FILE_PATH': '',
          'CHECKPOINT_TYPE': 'pytorch',
          'DATASET': 'ava',
          'DETECTIONS_PATH': '',
          'ENABLE': True,
          'GT_PATH': '',
          'NUM_ENSEMBLE_VIEWS': 10,
          'NUM_SPATIAL_CROPS': 3,
          'SAVE_RESULTS_PATH': ''},
 'TRAIN': {'AUTO_RESUME': False,
           'BATCH_SIZE': 4,
           'CHECKPOINT_CLEAR_NAME_PATTERN': (),
           'CHECKPOINT_EPOCH_RESET': False,
           'CHECKPOINT_FILE_PATH': '/srv/beegfs02/scratch/da_action/data/models_pretrained/SLOWFAST_32x2_R101_50_50.pkl',
           'CHECKPOINT_INFLATE': False,
           'CHECKPOINT_PERIOD': 1,
           'CHECKPOINT_TYPE': 'pytorch',
           'DATASET': 'ava',
           'ENABLE': True,
           'EVAL_PERIOD': 1},
 'X3D': {'BN_LIN5': False,
         'BOTTLENECK_FACTOR': 1.0,
         'CHANNELWISE_3x3x3': True,
         'DEPTH_FACTOR': 1.0,
         'DIM_C1': 12,
         'DIM_C5': 2048,
         'SCALE_RES2': False,
         'WIDTH_FACTOR': 1.0}}
0 frozen s1.pathway0_stem.conv.weight
1 frozen s1.pathway0_stem.bn.weight
2 frozen s1.pathway0_stem.bn.bias
3 frozen s1.pathway1_stem.conv.weight
4 frozen s1.pathway1_stem.bn.weight
5 frozen s1.pathway1_stem.bn.bias
6 frozen s1_fuse.conv_f2s.weight
7 frozen s1_fuse.bn.weight
8 frozen s1_fuse.bn.bias
9 frozen s2.pathway0_res0.branch1.weight
10 frozen s2.pathway0_res0.branch1_bn.weight
11 frozen s2.pathway0_res0.branch1_bn.bias
12 frozen s2.pathway0_res0.branch2.a.weight
13 frozen s2.pathway0_res0.branch2.a_bn.weight
14 frozen s2.pathway0_res0.branch2.a_bn.bias
15 frozen s2.pathway0_res0.branch2.b.weight
16 frozen s2.pathway0_res0.branch2.b_bn.weight
17 frozen s2.pathway0_res0.branch2.b_bn.bias
18 frozen s2.pathway0_res0.branch2.c.weight
19 frozen s2.pathway0_res0.branch2.c_bn.weight
20 frozen s2.pathway0_res0.branch2.c_bn.bias
21 frozen s2.pathway0_res1.branch2.a.weight
22 frozen s2.pathway0_res1.branch2.a_bn.weight
23 frozen s2.pathway0_res1.branch2.a_bn.bias
24 frozen s2.pathway0_res1.branch2.b.weight
25 frozen s2.pathway0_res1.branch2.b_bn.weight
26 frozen s2.pathway0_res1.branch2.b_bn.bias
27 frozen s2.pathway0_res1.branch2.c.weight
28 frozen s2.pathway0_res1.branch2.c_bn.weight
29 frozen s2.pathway0_res1.branch2.c_bn.bias
30 frozen s2.pathway0_res2.branch2.a.weight
31 frozen s2.pathway0_res2.branch2.a_bn.weight
32 frozen s2.pathway0_res2.branch2.a_bn.bias
33 frozen s2.pathway0_res2.branch2.b.weight
34 frozen s2.pathway0_res2.branch2.b_bn.weight
35 frozen s2.pathway0_res2.branch2.b_bn.bias
36 frozen s2.pathway0_res2.branch2.c.weight
37 frozen s2.pathway0_res2.branch2.c_bn.weight
38 frozen s2.pathway0_res2.branch2.c_bn.bias
39 frozen s2.pathway1_res0.branch1.weight
40 frozen s2.pathway1_res0.branch1_bn.weight
41 frozen s2.pathway1_res0.branch1_bn.bias
42 frozen s2.pathway1_res0.branch2.a.weight
43 frozen s2.pathway1_res0.branch2.a_bn.weight
44 frozen s2.pathway1_res0.branch2.a_bn.bias
45 frozen s2.pathway1_res0.branch2.b.weight
46 frozen s2.pathway1_res0.branch2.b_bn.weight
47 frozen s2.pathway1_res0.branch2.b_bn.bias
48 frozen s2.pathway1_res0.branch2.c.weight
49 frozen s2.pathway1_res0.branch2.c_bn.weight
50 frozen s2.pathway1_res0.branch2.c_bn.bias
51 frozen s2.pathway1_res1.branch2.a.weight
52 frozen s2.pathway1_res1.branch2.a_bn.weight
53 frozen s2.pathway1_res1.branch2.a_bn.bias
54 frozen s2.pathway1_res1.branch2.b.weight
55 frozen s2.pathway1_res1.branch2.b_bn.weight
56 frozen s2.pathway1_res1.branch2.b_bn.bias
57 frozen s2.pathway1_res1.branch2.c.weight
58 frozen s2.pathway1_res1.branch2.c_bn.weight
59 frozen s2.pathway1_res1.branch2.c_bn.bias
60 frozen s2.pathway1_res2.branch2.a.weight
61 frozen s2.pathway1_res2.branch2.a_bn.weight
62 frozen s2.pathway1_res2.branch2.a_bn.bias
63 frozen s2.pathway1_res2.branch2.b.weight
64 frozen s2.pathway1_res2.branch2.b_bn.weight
65 frozen s2.pathway1_res2.branch2.b_bn.bias
66 frozen s2.pathway1_res2.branch2.c.weight
67 frozen s2.pathway1_res2.branch2.c_bn.weight
68 frozen s2.pathway1_res2.branch2.c_bn.bias
69 frozen s2_fuse.conv_f2s.weight
70 frozen s2_fuse.bn.weight
71 frozen s2_fuse.bn.bias
72 frozen s3.pathway0_res0.branch1.weight
73 frozen s3.pathway0_res0.branch1_bn.weight
74 frozen s3.pathway0_res0.branch1_bn.bias
75 frozen s3.pathway0_res0.branch2.a.weight
76 frozen s3.pathway0_res0.branch2.a_bn.weight
77 frozen s3.pathway0_res0.branch2.a_bn.bias
78 frozen s3.pathway0_res0.branch2.b.weight
79 frozen s3.pathway0_res0.branch2.b_bn.weight
80 frozen s3.pathway0_res0.branch2.b_bn.bias
81 frozen s3.pathway0_res0.branch2.c.weight
82 frozen s3.pathway0_res0.branch2.c_bn.weight
83 frozen s3.pathway0_res0.branch2.c_bn.bias
84 frozen s3.pathway0_res1.branch2.a.weight
85 frozen s3.pathway0_res1.branch2.a_bn.weight
86 frozen s3.pathway0_res1.branch2.a_bn.bias
87 frozen s3.pathway0_res1.branch2.b.weight
88 frozen s3.pathway0_res1.branch2.b_bn.weight
89 frozen s3.pathway0_res1.branch2.b_bn.bias
90 frozen s3.pathway0_res1.branch2.c.weight
91 frozen s3.pathway0_res1.branch2.c_bn.weight
92 frozen s3.pathway0_res1.branch2.c_bn.bias
93 frozen s3.pathway0_res2.branch2.a.weight
94 frozen s3.pathway0_res2.branch2.a_bn.weight
95 frozen s3.pathway0_res2.branch2.a_bn.bias
96 frozen s3.pathway0_res2.branch2.b.weight
97 frozen s3.pathway0_res2.branch2.b_bn.weight
98 frozen s3.pathway0_res2.branch2.b_bn.bias
99 frozen s3.pathway0_res2.branch2.c.weight
100 frozen s3.pathway0_res2.branch2.c_bn.weight
101 frozen s3.pathway0_res2.branch2.c_bn.bias
102 frozen s3.pathway0_res3.branch2.a.weight
103 frozen s3.pathway0_res3.branch2.a_bn.weight
104 frozen s3.pathway0_res3.branch2.a_bn.bias
105 frozen s3.pathway0_res3.branch2.b.weight
106 frozen s3.pathway0_res3.branch2.b_bn.weight
107 frozen s3.pathway0_res3.branch2.b_bn.bias
108 frozen s3.pathway0_res3.branch2.c.weight
109 frozen s3.pathway0_res3.branch2.c_bn.weight
110 frozen s3.pathway0_res3.branch2.c_bn.bias
111 frozen s3.pathway1_res0.branch1.weight
112 frozen s3.pathway1_res0.branch1_bn.weight
113 frozen s3.pathway1_res0.branch1_bn.bias
114 frozen s3.pathway1_res0.branch2.a.weight
115 frozen s3.pathway1_res0.branch2.a_bn.weight
116 frozen s3.pathway1_res0.branch2.a_bn.bias
117 frozen s3.pathway1_res0.branch2.b.weight
118 frozen s3.pathway1_res0.branch2.b_bn.weight
119 frozen s3.pathway1_res0.branch2.b_bn.bias
120 frozen s3.pathway1_res0.branch2.c.weight
121 frozen s3.pathway1_res0.branch2.c_bn.weight
122 frozen s3.pathway1_res0.branch2.c_bn.bias
123 frozen s3.pathway1_res1.branch2.a.weight
124 frozen s3.pathway1_res1.branch2.a_bn.weight
125 frozen s3.pathway1_res1.branch2.a_bn.bias
126 frozen s3.pathway1_res1.branch2.b.weight
127 frozen s3.pathway1_res1.branch2.b_bn.weight
128 frozen s3.pathway1_res1.branch2.b_bn.bias
129 frozen s3.pathway1_res1.branch2.c.weight
130 frozen s3.pathway1_res1.branch2.c_bn.weight
131 frozen s3.pathway1_res1.branch2.c_bn.bias
132 frozen s3.pathway1_res2.branch2.a.weight
133 frozen s3.pathway1_res2.branch2.a_bn.weight
134 frozen s3.pathway1_res2.branch2.a_bn.bias
135 frozen s3.pathway1_res2.branch2.b.weight
136 frozen s3.pathway1_res2.branch2.b_bn.weight
137 frozen s3.pathway1_res2.branch2.b_bn.bias
138 frozen s3.pathway1_res2.branch2.c.weight
139 frozen s3.pathway1_res2.branch2.c_bn.weight
140 frozen s3.pathway1_res2.branch2.c_bn.bias
141 frozen s3.pathway1_res3.branch2.a.weight
142 frozen s3.pathway1_res3.branch2.a_bn.weight
143 frozen s3.pathway1_res3.branch2.a_bn.bias
144 frozen s3.pathway1_res3.branch2.b.weight
145 frozen s3.pathway1_res3.branch2.b_bn.weight
146 frozen s3.pathway1_res3.branch2.b_bn.bias
147 frozen s3.pathway1_res3.branch2.c.weight
148 frozen s3.pathway1_res3.branch2.c_bn.weight
149 frozen s3.pathway1_res3.branch2.c_bn.bias
150 frozen s3_fuse.conv_f2s.weight
151 frozen s3_fuse.bn.weight
152 frozen s3_fuse.bn.bias
153 frozen s4.pathway0_res0.branch1.weight
154 frozen s4.pathway0_res0.branch1_bn.weight
155 frozen s4.pathway0_res0.branch1_bn.bias
156 frozen s4.pathway0_res0.branch2.a.weight
157 frozen s4.pathway0_res0.branch2.a_bn.weight
158 frozen s4.pathway0_res0.branch2.a_bn.bias
159 frozen s4.pathway0_res0.branch2.b.weight
160 frozen s4.pathway0_res0.branch2.b_bn.weight
161 frozen s4.pathway0_res0.branch2.b_bn.bias
162 frozen s4.pathway0_res0.branch2.c.weight
163 frozen s4.pathway0_res0.branch2.c_bn.weight
164 frozen s4.pathway0_res0.branch2.c_bn.bias
165 frozen s4.pathway0_res1.branch2.a.weight
166 frozen s4.pathway0_res1.branch2.a_bn.weight
167 frozen s4.pathway0_res1.branch2.a_bn.bias
168 frozen s4.pathway0_res1.branch2.b.weight
169 frozen s4.pathway0_res1.branch2.b_bn.weight
170 frozen s4.pathway0_res1.branch2.b_bn.bias
171 frozen s4.pathway0_res1.branch2.c.weight
172 frozen s4.pathway0_res1.branch2.c_bn.weight
173 frozen s4.pathway0_res1.branch2.c_bn.bias
174 frozen s4.pathway0_res2.branch2.a.weight
175 frozen s4.pathway0_res2.branch2.a_bn.weight
176 frozen s4.pathway0_res2.branch2.a_bn.bias
177 frozen s4.pathway0_res2.branch2.b.weight
178 frozen s4.pathway0_res2.branch2.b_bn.weight
179 frozen s4.pathway0_res2.branch2.b_bn.bias
180 frozen s4.pathway0_res2.branch2.c.weight
181 frozen s4.pathway0_res2.branch2.c_bn.weight
182 frozen s4.pathway0_res2.branch2.c_bn.bias
183 frozen s4.pathway0_res3.branch2.a.weight
184 frozen s4.pathway0_res3.branch2.a_bn.weight
185 frozen s4.pathway0_res3.branch2.a_bn.bias
186 frozen s4.pathway0_res3.branch2.b.weight
187 frozen s4.pathway0_res3.branch2.b_bn.weight
188 frozen s4.pathway0_res3.branch2.b_bn.bias
189 frozen s4.pathway0_res3.branch2.c.weight
190 frozen s4.pathway0_res3.branch2.c_bn.weight
191 frozen s4.pathway0_res3.branch2.c_bn.bias
192 frozen s4.pathway0_res4.branch2.a.weight
193 frozen s4.pathway0_res4.branch2.a_bn.weight
194 frozen s4.pathway0_res4.branch2.a_bn.bias
195 frozen s4.pathway0_res4.branch2.b.weight
196 frozen s4.pathway0_res4.branch2.b_bn.weight
197 frozen s4.pathway0_res4.branch2.b_bn.bias
198 frozen s4.pathway0_res4.branch2.c.weight
199 frozen s4.pathway0_res4.branch2.c_bn.weight
200 frozen s4.pathway0_res4.branch2.c_bn.bias
201 frozen s4.pathway0_res5.branch2.a.weight
202 frozen s4.pathway0_res5.branch2.a_bn.weight
203 frozen s4.pathway0_res5.branch2.a_bn.bias
204 frozen s4.pathway0_res5.branch2.b.weight
205 frozen s4.pathway0_res5.branch2.b_bn.weight
206 frozen s4.pathway0_res5.branch2.b_bn.bias
207 frozen s4.pathway0_res5.branch2.c.weight
208 frozen s4.pathway0_res5.branch2.c_bn.weight
209 frozen s4.pathway0_res5.branch2.c_bn.bias
210 frozen s4.pathway0_res6.branch2.a.weight
211 frozen s4.pathway0_res6.branch2.a_bn.weight
212 frozen s4.pathway0_res6.branch2.a_bn.bias
213 frozen s4.pathway0_res6.branch2.b.weight
214 frozen s4.pathway0_res6.branch2.b_bn.weight
215 frozen s4.pathway0_res6.branch2.b_bn.bias
216 frozen s4.pathway0_res6.branch2.c.weight
217 frozen s4.pathway0_res6.branch2.c_bn.weight
218 frozen s4.pathway0_res6.branch2.c_bn.bias
219 frozen s4.pathway0_nonlocal6.conv_theta.weight
220 frozen s4.pathway0_nonlocal6.conv_theta.bias
221 frozen s4.pathway0_nonlocal6.conv_phi.weight
222 frozen s4.pathway0_nonlocal6.conv_phi.bias
223 frozen s4.pathway0_nonlocal6.conv_g.weight
224 frozen s4.pathway0_nonlocal6.conv_g.bias
225 frozen s4.pathway0_nonlocal6.conv_out.weight
226 frozen s4.pathway0_nonlocal6.conv_out.bias
227 frozen s4.pathway0_nonlocal6.bn.weight
228 frozen s4.pathway0_nonlocal6.bn.bias
229 frozen s4.pathway0_res7.branch2.a.weight
230 frozen s4.pathway0_res7.branch2.a_bn.weight
231 frozen s4.pathway0_res7.branch2.a_bn.bias
232 frozen s4.pathway0_res7.branch2.b.weight
233 frozen s4.pathway0_res7.branch2.b_bn.weight
234 frozen s4.pathway0_res7.branch2.b_bn.bias
235 frozen s4.pathway0_res7.branch2.c.weight
236 frozen s4.pathway0_res7.branch2.c_bn.weight
237 frozen s4.pathway0_res7.branch2.c_bn.bias
238 frozen s4.pathway0_res8.branch2.a.weight
239 frozen s4.pathway0_res8.branch2.a_bn.weight
240 frozen s4.pathway0_res8.branch2.a_bn.bias
241 frozen s4.pathway0_res8.branch2.b.weight
242 frozen s4.pathway0_res8.branch2.b_bn.weight
243 frozen s4.pathway0_res8.branch2.b_bn.bias
244 frozen s4.pathway0_res8.branch2.c.weight
245 frozen s4.pathway0_res8.branch2.c_bn.weight
246 frozen s4.pathway0_res8.branch2.c_bn.bias
247 frozen s4.pathway0_res9.branch2.a.weight
248 frozen s4.pathway0_res9.branch2.a_bn.weight
249 frozen s4.pathway0_res9.branch2.a_bn.bias
250 frozen s4.pathway0_res9.branch2.b.weight
251 frozen s4.pathway0_res9.branch2.b_bn.weight
252 frozen s4.pathway0_res9.branch2.b_bn.bias
253 frozen s4.pathway0_res9.branch2.c.weight
254 frozen s4.pathway0_res9.branch2.c_bn.weight
255 frozen s4.pathway0_res9.branch2.c_bn.bias
256 frozen s4.pathway0_res10.branch2.a.weight
257 frozen s4.pathway0_res10.branch2.a_bn.weight
258 frozen s4.pathway0_res10.branch2.a_bn.bias
259 frozen s4.pathway0_res10.branch2.b.weight
260 frozen s4.pathway0_res10.branch2.b_bn.weight
261 frozen s4.pathway0_res10.branch2.b_bn.bias
262 frozen s4.pathway0_res10.branch2.c.weight
263 frozen s4.pathway0_res10.branch2.c_bn.weight
264 frozen s4.pathway0_res10.branch2.c_bn.bias
265 frozen s4.pathway0_res11.branch2.a.weight
266 frozen s4.pathway0_res11.branch2.a_bn.weight
267 frozen s4.pathway0_res11.branch2.a_bn.bias
268 frozen s4.pathway0_res11.branch2.b.weight
269 frozen s4.pathway0_res11.branch2.b_bn.weight
270 frozen s4.pathway0_res11.branch2.b_bn.bias
271 frozen s4.pathway0_res11.branch2.c.weight
272 frozen s4.pathway0_res11.branch2.c_bn.weight
273 frozen s4.pathway0_res11.branch2.c_bn.bias
274 frozen s4.pathway0_res12.branch2.a.weight
275 frozen s4.pathway0_res12.branch2.a_bn.weight
276 frozen s4.pathway0_res12.branch2.a_bn.bias
277 frozen s4.pathway0_res12.branch2.b.weight
278 frozen s4.pathway0_res12.branch2.b_bn.weight
279 frozen s4.pathway0_res12.branch2.b_bn.bias
280 frozen s4.pathway0_res12.branch2.c.weight
281 frozen s4.pathway0_res12.branch2.c_bn.weight
282 frozen s4.pathway0_res12.branch2.c_bn.bias
283 frozen s4.pathway0_res13.branch2.a.weight
284 frozen s4.pathway0_res13.branch2.a_bn.weight
285 frozen s4.pathway0_res13.branch2.a_bn.bias
286 frozen s4.pathway0_res13.branch2.b.weight
287 frozen s4.pathway0_res13.branch2.b_bn.weight
288 frozen s4.pathway0_res13.branch2.b_bn.bias
289 frozen s4.pathway0_res13.branch2.c.weight
290 frozen s4.pathway0_res13.branch2.c_bn.weight
291 frozen s4.pathway0_res13.branch2.c_bn.bias
292 frozen s4.pathway0_nonlocal13.conv_theta.weight
293 frozen s4.pathway0_nonlocal13.conv_theta.bias
294 frozen s4.pathway0_nonlocal13.conv_phi.weight
295 frozen s4.pathway0_nonlocal13.conv_phi.bias
296 frozen s4.pathway0_nonlocal13.conv_g.weight
297 frozen s4.pathway0_nonlocal13.conv_g.bias
298 frozen s4.pathway0_nonlocal13.conv_out.weight
299 frozen s4.pathway0_nonlocal13.conv_out.bias
300 frozen s4.pathway0_nonlocal13.bn.weight
301 frozen s4.pathway0_nonlocal13.bn.bias
302 frozen s4.pathway0_res14.branch2.a.weight
303 frozen s4.pathway0_res14.branch2.a_bn.weight
304 frozen s4.pathway0_res14.branch2.a_bn.bias
305 frozen s4.pathway0_res14.branch2.b.weight
306 frozen s4.pathway0_res14.branch2.b_bn.weight
307 frozen s4.pathway0_res14.branch2.b_bn.bias
308 frozen s4.pathway0_res14.branch2.c.weight
309 frozen s4.pathway0_res14.branch2.c_bn.weight
310 frozen s4.pathway0_res14.branch2.c_bn.bias
311 frozen s4.pathway0_res15.branch2.a.weight
312 frozen s4.pathway0_res15.branch2.a_bn.weight
313 frozen s4.pathway0_res15.branch2.a_bn.bias
314 frozen s4.pathway0_res15.branch2.b.weight
315 frozen s4.pathway0_res15.branch2.b_bn.weight
316 frozen s4.pathway0_res15.branch2.b_bn.bias
317 frozen s4.pathway0_res15.branch2.c.weight
318 frozen s4.pathway0_res15.branch2.c_bn.weight
319 frozen s4.pathway0_res15.branch2.c_bn.bias
320 frozen s4.pathway0_res16.branch2.a.weight
321 frozen s4.pathway0_res16.branch2.a_bn.weight
322 frozen s4.pathway0_res16.branch2.a_bn.bias
323 frozen s4.pathway0_res16.branch2.b.weight
324 frozen s4.pathway0_res16.branch2.b_bn.weight
325 frozen s4.pathway0_res16.branch2.b_bn.bias
326 frozen s4.pathway0_res16.branch2.c.weight
327 frozen s4.pathway0_res16.branch2.c_bn.weight
328 frozen s4.pathway0_res16.branch2.c_bn.bias
329 frozen s4.pathway0_res17.branch2.a.weight
330 frozen s4.pathway0_res17.branch2.a_bn.weight
331 frozen s4.pathway0_res17.branch2.a_bn.bias
332 frozen s4.pathway0_res17.branch2.b.weight
333 frozen s4.pathway0_res17.branch2.b_bn.weight
334 frozen s4.pathway0_res17.branch2.b_bn.bias
335 frozen s4.pathway0_res17.branch2.c.weight
336 frozen s4.pathway0_res17.branch2.c_bn.weight
337 frozen s4.pathway0_res17.branch2.c_bn.bias
338 frozen s4.pathway0_res18.branch2.a.weight
339 frozen s4.pathway0_res18.branch2.a_bn.weight
340 frozen s4.pathway0_res18.branch2.a_bn.bias
341 frozen s4.pathway0_res18.branch2.b.weight
342 frozen s4.pathway0_res18.branch2.b_bn.weight
343 frozen s4.pathway0_res18.branch2.b_bn.bias
344 frozen s4.pathway0_res18.branch2.c.weight
345 frozen s4.pathway0_res18.branch2.c_bn.weight
346 frozen s4.pathway0_res18.branch2.c_bn.bias
347 frozen s4.pathway0_res19.branch2.a.weight
348 frozen s4.pathway0_res19.branch2.a_bn.weight
349 frozen s4.pathway0_res19.branch2.a_bn.bias
350 frozen s4.pathway0_res19.branch2.b.weight
351 frozen s4.pathway0_res19.branch2.b_bn.weight
352 frozen s4.pathway0_res19.branch2.b_bn.bias
353 frozen s4.pathway0_res19.branch2.c.weight
354 frozen s4.pathway0_res19.branch2.c_bn.weight
355 frozen s4.pathway0_res19.branch2.c_bn.bias
356 frozen s4.pathway0_res20.branch2.a.weight
357 frozen s4.pathway0_res20.branch2.a_bn.weight
358 frozen s4.pathway0_res20.branch2.a_bn.bias
359 frozen s4.pathway0_res20.branch2.b.weight
360 frozen s4.pathway0_res20.branch2.b_bn.weight
361 frozen s4.pathway0_res20.branch2.b_bn.bias
362 frozen s4.pathway0_res20.branch2.c.weight
363 frozen s4.pathway0_res20.branch2.c_bn.weight
364 frozen s4.pathway0_res20.branch2.c_bn.bias
365 frozen s4.pathway0_nonlocal20.conv_theta.weight
366 frozen s4.pathway0_nonlocal20.conv_theta.bias
367 frozen s4.pathway0_nonlocal20.conv_phi.weight
368 frozen s4.pathway0_nonlocal20.conv_phi.bias
369 frozen s4.pathway0_nonlocal20.conv_g.weight
370 frozen s4.pathway0_nonlocal20.conv_g.bias
371 frozen s4.pathway0_nonlocal20.conv_out.weight
372 frozen s4.pathway0_nonlocal20.conv_out.bias
373 frozen s4.pathway0_nonlocal20.bn.weight
374 frozen s4.pathway0_nonlocal20.bn.bias
375 frozen s4.pathway0_res21.branch2.a.weight
376 frozen s4.pathway0_res21.branch2.a_bn.weight
377 frozen s4.pathway0_res21.branch2.a_bn.bias
378 frozen s4.pathway0_res21.branch2.b.weight
379 frozen s4.pathway0_res21.branch2.b_bn.weight
380 frozen s4.pathway0_res21.branch2.b_bn.bias
381 frozen s4.pathway0_res21.branch2.c.weight
382 frozen s4.pathway0_res21.branch2.c_bn.weight
383 frozen s4.pathway0_res21.branch2.c_bn.bias
384 frozen s4.pathway0_res22.branch2.a.weight
385 frozen s4.pathway0_res22.branch2.a_bn.weight
386 frozen s4.pathway0_res22.branch2.a_bn.bias
387 frozen s4.pathway0_res22.branch2.b.weight
388 frozen s4.pathway0_res22.branch2.b_bn.weight
389 frozen s4.pathway0_res22.branch2.b_bn.bias
390 frozen s4.pathway0_res22.branch2.c.weight
391 frozen s4.pathway0_res22.branch2.c_bn.weight
392 frozen s4.pathway0_res22.branch2.c_bn.bias
393 frozen s4.pathway1_res0.branch1.weight
394 frozen s4.pathway1_res0.branch1_bn.weight
395 frozen s4.pathway1_res0.branch1_bn.bias
396 frozen s4.pathway1_res0.branch2.a.weight
397 frozen s4.pathway1_res0.branch2.a_bn.weight
398 frozen s4.pathway1_res0.branch2.a_bn.bias
399 frozen s4.pathway1_res0.branch2.b.weight
400 frozen s4.pathway1_res0.branch2.b_bn.weight
401 frozen s4.pathway1_res0.branch2.b_bn.bias
402 frozen s4.pathway1_res0.branch2.c.weight
403 frozen s4.pathway1_res0.branch2.c_bn.weight
404 frozen s4.pathway1_res0.branch2.c_bn.bias
405 frozen s4.pathway1_res1.branch2.a.weight
406 frozen s4.pathway1_res1.branch2.a_bn.weight
407 frozen s4.pathway1_res1.branch2.a_bn.bias
408 frozen s4.pathway1_res1.branch2.b.weight
409 frozen s4.pathway1_res1.branch2.b_bn.weight
410 frozen s4.pathway1_res1.branch2.b_bn.bias
411 frozen s4.pathway1_res1.branch2.c.weight
412 frozen s4.pathway1_res1.branch2.c_bn.weight
413 frozen s4.pathway1_res1.branch2.c_bn.bias
414 frozen s4.pathway1_res2.branch2.a.weight
415 frozen s4.pathway1_res2.branch2.a_bn.weight
416 frozen s4.pathway1_res2.branch2.a_bn.bias
417 frozen s4.pathway1_res2.branch2.b.weight
418 frozen s4.pathway1_res2.branch2.b_bn.weight
419 frozen s4.pathway1_res2.branch2.b_bn.bias
420 frozen s4.pathway1_res2.branch2.c.weight
421 frozen s4.pathway1_res2.branch2.c_bn.weight
422 frozen s4.pathway1_res2.branch2.c_bn.bias
423 frozen s4.pathway1_res3.branch2.a.weight
424 frozen s4.pathway1_res3.branch2.a_bn.weight
425 frozen s4.pathway1_res3.branch2.a_bn.bias
426 frozen s4.pathway1_res3.branch2.b.weight
427 frozen s4.pathway1_res3.branch2.b_bn.weight
428 frozen s4.pathway1_res3.branch2.b_bn.bias
429 frozen s4.pathway1_res3.branch2.c.weight
430 frozen s4.pathway1_res3.branch2.c_bn.weight
431 frozen s4.pathway1_res3.branch2.c_bn.bias
432 frozen s4.pathway1_res4.branch2.a.weight
433 frozen s4.pathway1_res4.branch2.a_bn.weight
434 frozen s4.pathway1_res4.branch2.a_bn.bias
435 frozen s4.pathway1_res4.branch2.b.weight
436 frozen s4.pathway1_res4.branch2.b_bn.weight
437 frozen s4.pathway1_res4.branch2.b_bn.bias
438 frozen s4.pathway1_res4.branch2.c.weight
439 frozen s4.pathway1_res4.branch2.c_bn.weight
440 frozen s4.pathway1_res4.branch2.c_bn.bias
441 frozen s4.pathway1_res5.branch2.a.weight
442 frozen s4.pathway1_res5.branch2.a_bn.weight
443 frozen s4.pathway1_res5.branch2.a_bn.bias
444 frozen s4.pathway1_res5.branch2.b.weight
445 frozen s4.pathway1_res5.branch2.b_bn.weight
446 frozen s4.pathway1_res5.branch2.b_bn.bias
447 frozen s4.pathway1_res5.branch2.c.weight
448 frozen s4.pathway1_res5.branch2.c_bn.weight
449 frozen s4.pathway1_res5.branch2.c_bn.bias
450 frozen s4.pathway1_res6.branch2.a.weight
451 frozen s4.pathway1_res6.branch2.a_bn.weight
452 frozen s4.pathway1_res6.branch2.a_bn.bias
453 frozen s4.pathway1_res6.branch2.b.weight
454 frozen s4.pathway1_res6.branch2.b_bn.weight
455 frozen s4.pathway1_res6.branch2.b_bn.bias
456 frozen s4.pathway1_res6.branch2.c.weight
457 frozen s4.pathway1_res6.branch2.c_bn.weight
458 frozen s4.pathway1_res6.branch2.c_bn.bias
459 frozen s4.pathway1_res7.branch2.a.weight
460 frozen s4.pathway1_res7.branch2.a_bn.weight
461 frozen s4.pathway1_res7.branch2.a_bn.bias
462 frozen s4.pathway1_res7.branch2.b.weight
463 frozen s4.pathway1_res7.branch2.b_bn.weight
464 frozen s4.pathway1_res7.branch2.b_bn.bias
465 frozen s4.pathway1_res7.branch2.c.weight
466 frozen s4.pathway1_res7.branch2.c_bn.weight
467 frozen s4.pathway1_res7.branch2.c_bn.bias
468 frozen s4.pathway1_res8.branch2.a.weight
469 frozen s4.pathway1_res8.branch2.a_bn.weight
470 frozen s4.pathway1_res8.branch2.a_bn.bias
471 frozen s4.pathway1_res8.branch2.b.weight
472 frozen s4.pathway1_res8.branch2.b_bn.weight
473 frozen s4.pathway1_res8.branch2.b_bn.bias
474 frozen s4.pathway1_res8.branch2.c.weight
475 frozen s4.pathway1_res8.branch2.c_bn.weight
476 frozen s4.pathway1_res8.branch2.c_bn.bias
477 frozen s4.pathway1_res9.branch2.a.weight
478 frozen s4.pathway1_res9.branch2.a_bn.weight
479 frozen s4.pathway1_res9.branch2.a_bn.bias
480 frozen s4.pathway1_res9.branch2.b.weight
481 frozen s4.pathway1_res9.branch2.b_bn.weight
482 frozen s4.pathway1_res9.branch2.b_bn.bias
483 frozen s4.pathway1_res9.branch2.c.weight
484 frozen s4.pathway1_res9.branch2.c_bn.weight
485 frozen s4.pathway1_res9.branch2.c_bn.bias
486 frozen s4.pathway1_res10.branch2.a.weight
487 frozen s4.pathway1_res10.branch2.a_bn.weight
488 frozen s4.pathway1_res10.branch2.a_bn.bias
489 frozen s4.pathway1_res10.branch2.b.weight
490 frozen s4.pathway1_res10.branch2.b_bn.weight
491 frozen s4.pathway1_res10.branch2.b_bn.bias
492 frozen s4.pathway1_res10.branch2.c.weight
493 frozen s4.pathway1_res10.branch2.c_bn.weight
494 frozen s4.pathway1_res10.branch2.c_bn.bias
495 frozen s4.pathway1_res11.branch2.a.weight
496 frozen s4.pathway1_res11.branch2.a_bn.weight
497 frozen s4.pathway1_res11.branch2.a_bn.bias
498 frozen s4.pathway1_res11.branch2.b.weight
499 frozen s4.pathway1_res11.branch2.b_bn.weight
500 frozen s4.pathway1_res11.branch2.b_bn.bias
501 frozen s4.pathway1_res11.branch2.c.weight
502 frozen s4.pathway1_res11.branch2.c_bn.weight
503 frozen s4.pathway1_res11.branch2.c_bn.bias
504 frozen s4.pathway1_res12.branch2.a.weight
505 frozen s4.pathway1_res12.branch2.a_bn.weight
506 frozen s4.pathway1_res12.branch2.a_bn.bias
507 frozen s4.pathway1_res12.branch2.b.weight
508 frozen s4.pathway1_res12.branch2.b_bn.weight
509 frozen s4.pathway1_res12.branch2.b_bn.bias
510 frozen s4.pathway1_res12.branch2.c.weight
511 frozen s4.pathway1_res12.branch2.c_bn.weight
512 frozen s4.pathway1_res12.branch2.c_bn.bias
513 frozen s4.pathway1_res13.branch2.a.weight
514 frozen s4.pathway1_res13.branch2.a_bn.weight
515 frozen s4.pathway1_res13.branch2.a_bn.bias
516 frozen s4.pathway1_res13.branch2.b.weight
517 frozen s4.pathway1_res13.branch2.b_bn.weight
518 frozen s4.pathway1_res13.branch2.b_bn.bias
519 frozen s4.pathway1_res13.branch2.c.weight
520 frozen s4.pathway1_res13.branch2.c_bn.weight
521 frozen s4.pathway1_res13.branch2.c_bn.bias
522 frozen s4.pathway1_res14.branch2.a.weight
523 frozen s4.pathway1_res14.branch2.a_bn.weight
524 frozen s4.pathway1_res14.branch2.a_bn.bias
525 frozen s4.pathway1_res14.branch2.b.weight
526 frozen s4.pathway1_res14.branch2.b_bn.weight
527 frozen s4.pathway1_res14.branch2.b_bn.bias
528 frozen s4.pathway1_res14.branch2.c.weight
529 frozen s4.pathway1_res14.branch2.c_bn.weight
530 frozen s4.pathway1_res14.branch2.c_bn.bias
531 frozen s4.pathway1_res15.branch2.a.weight
532 frozen s4.pathway1_res15.branch2.a_bn.weight
533 frozen s4.pathway1_res15.branch2.a_bn.bias
534 frozen s4.pathway1_res15.branch2.b.weight
535 frozen s4.pathway1_res15.branch2.b_bn.weight
536 frozen s4.pathway1_res15.branch2.b_bn.bias
537 frozen s4.pathway1_res15.branch2.c.weight
538 frozen s4.pathway1_res15.branch2.c_bn.weight
539 frozen s4.pathway1_res15.branch2.c_bn.bias
540 frozen s4.pathway1_res16.branch2.a.weight
541 frozen s4.pathway1_res16.branch2.a_bn.weight
542 frozen s4.pathway1_res16.branch2.a_bn.bias
543 frozen s4.pathway1_res16.branch2.b.weight
544 frozen s4.pathway1_res16.branch2.b_bn.weight
545 frozen s4.pathway1_res16.branch2.b_bn.bias
546 frozen s4.pathway1_res16.branch2.c.weight
547 frozen s4.pathway1_res16.branch2.c_bn.weight
548 frozen s4.pathway1_res16.branch2.c_bn.bias
549 frozen s4.pathway1_res17.branch2.a.weight
550 frozen s4.pathway1_res17.branch2.a_bn.weight
551 frozen s4.pathway1_res17.branch2.a_bn.bias
552 frozen s4.pathway1_res17.branch2.b.weight
553 frozen s4.pathway1_res17.branch2.b_bn.weight
554 frozen s4.pathway1_res17.branch2.b_bn.bias
555 frozen s4.pathway1_res17.branch2.c.weight
556 frozen s4.pathway1_res17.branch2.c_bn.weight
557 frozen s4.pathway1_res17.branch2.c_bn.bias
558 frozen s4.pathway1_res18.branch2.a.weight
559 frozen s4.pathway1_res18.branch2.a_bn.weight
560 frozen s4.pathway1_res18.branch2.a_bn.bias
561 frozen s4.pathway1_res18.branch2.b.weight
562 frozen s4.pathway1_res18.branch2.b_bn.weight
563 frozen s4.pathway1_res18.branch2.b_bn.bias
564 frozen s4.pathway1_res18.branch2.c.weight
565 frozen s4.pathway1_res18.branch2.c_bn.weight
566 frozen s4.pathway1_res18.branch2.c_bn.bias
567 frozen s4.pathway1_res19.branch2.a.weight
568 frozen s4.pathway1_res19.branch2.a_bn.weight
569 frozen s4.pathway1_res19.branch2.a_bn.bias
570 frozen s4.pathway1_res19.branch2.b.weight
571 frozen s4.pathway1_res19.branch2.b_bn.weight
572 frozen s4.pathway1_res19.branch2.b_bn.bias
573 frozen s4.pathway1_res19.branch2.c.weight
574 frozen s4.pathway1_res19.branch2.c_bn.weight
575 frozen s4.pathway1_res19.branch2.c_bn.bias
576 frozen s4.pathway1_res20.branch2.a.weight
577 frozen s4.pathway1_res20.branch2.a_bn.weight
578 frozen s4.pathway1_res20.branch2.a_bn.bias
579 frozen s4.pathway1_res20.branch2.b.weight
580 frozen s4.pathway1_res20.branch2.b_bn.weight
581 frozen s4.pathway1_res20.branch2.b_bn.bias
582 frozen s4.pathway1_res20.branch2.c.weight
583 frozen s4.pathway1_res20.branch2.c_bn.weight
584 frozen s4.pathway1_res20.branch2.c_bn.bias
585 frozen s4.pathway1_res21.branch2.a.weight
586 frozen s4.pathway1_res21.branch2.a_bn.weight
587 frozen s4.pathway1_res21.branch2.a_bn.bias
588 frozen s4.pathway1_res21.branch2.b.weight
589 frozen s4.pathway1_res21.branch2.b_bn.weight
590 frozen s4.pathway1_res21.branch2.b_bn.bias
591 frozen s4.pathway1_res21.branch2.c.weight
592 frozen s4.pathway1_res21.branch2.c_bn.weight
593 frozen s4.pathway1_res21.branch2.c_bn.bias
594 frozen s4.pathway1_res22.branch2.a.weight
595 frozen s4.pathway1_res22.branch2.a_bn.weight
596 frozen s4.pathway1_res22.branch2.a_bn.bias
597 frozen s4.pathway1_res22.branch2.b.weight
598 frozen s4.pathway1_res22.branch2.b_bn.weight
599 frozen s4.pathway1_res22.branch2.b_bn.bias
600 frozen s4.pathway1_res22.branch2.c.weight
601 frozen s4.pathway1_res22.branch2.c_bn.weight
602 frozen s4.pathway1_res22.branch2.c_bn.bias
603 frozen s4_fuse.conv_f2s.weight
604 frozen s4_fuse.bn.weight
605 frozen s4_fuse.bn.bias
606 unfrozen s5.pathway0_res0.branch1.weight
607 unfrozen s5.pathway0_res0.branch1_bn.weight
608 unfrozen s5.pathway0_res0.branch1_bn.bias
609 unfrozen s5.pathway0_res0.branch2.a.weight
610 unfrozen s5.pathway0_res0.branch2.a_bn.weight
611 unfrozen s5.pathway0_res0.branch2.a_bn.bias
612 unfrozen s5.pathway0_res0.branch2.b.weight
613 unfrozen s5.pathway0_res0.branch2.b_bn.weight
614 unfrozen s5.pathway0_res0.branch2.b_bn.bias
615 unfrozen s5.pathway0_res0.branch2.c.weight
616 unfrozen s5.pathway0_res0.branch2.c_bn.weight
617 unfrozen s5.pathway0_res0.branch2.c_bn.bias
618 unfrozen s5.pathway0_res1.branch2.a.weight
619 unfrozen s5.pathway0_res1.branch2.a_bn.weight
620 unfrozen s5.pathway0_res1.branch2.a_bn.bias
621 unfrozen s5.pathway0_res1.branch2.b.weight
622 unfrozen s5.pathway0_res1.branch2.b_bn.weight
623 unfrozen s5.pathway0_res1.branch2.b_bn.bias
624 unfrozen s5.pathway0_res1.branch2.c.weight
625 unfrozen s5.pathway0_res1.branch2.c_bn.weight
626 unfrozen s5.pathway0_res1.branch2.c_bn.bias
627 unfrozen s5.pathway0_res2.branch2.a.weight
628 unfrozen s5.pathway0_res2.branch2.a_bn.weight
629 unfrozen s5.pathway0_res2.branch2.a_bn.bias
630 unfrozen s5.pathway0_res2.branch2.b.weight
631 unfrozen s5.pathway0_res2.branch2.b_bn.weight
632 unfrozen s5.pathway0_res2.branch2.b_bn.bias
633 unfrozen s5.pathway0_res2.branch2.c.weight
634 unfrozen s5.pathway0_res2.branch2.c_bn.weight
635 unfrozen s5.pathway0_res2.branch2.c_bn.bias
636 unfrozen s5.pathway1_res0.branch1.weight
637 unfrozen s5.pathway1_res0.branch1_bn.weight
638 unfrozen s5.pathway1_res0.branch1_bn.bias
639 unfrozen s5.pathway1_res0.branch2.a.weight
640 unfrozen s5.pathway1_res0.branch2.a_bn.weight
641 unfrozen s5.pathway1_res0.branch2.a_bn.bias
642 unfrozen s5.pathway1_res0.branch2.b.weight
643 unfrozen s5.pathway1_res0.branch2.b_bn.weight
644 unfrozen s5.pathway1_res0.branch2.b_bn.bias
645 unfrozen s5.pathway1_res0.branch2.c.weight
646 unfrozen s5.pathway1_res0.branch2.c_bn.weight
647 unfrozen s5.pathway1_res0.branch2.c_bn.bias
648 unfrozen s5.pathway1_res1.branch2.a.weight
649 unfrozen s5.pathway1_res1.branch2.a_bn.weight
650 unfrozen s5.pathway1_res1.branch2.a_bn.bias
651 unfrozen s5.pathway1_res1.branch2.b.weight
652 unfrozen s5.pathway1_res1.branch2.b_bn.weight
653 unfrozen s5.pathway1_res1.branch2.b_bn.bias
654 unfrozen s5.pathway1_res1.branch2.c.weight
655 unfrozen s5.pathway1_res1.branch2.c_bn.weight
656 unfrozen s5.pathway1_res1.branch2.c_bn.bias
657 unfrozen s5.pathway1_res2.branch2.a.weight
658 unfrozen s5.pathway1_res2.branch2.a_bn.weight
659 unfrozen s5.pathway1_res2.branch2.a_bn.bias
660 unfrozen s5.pathway1_res2.branch2.b.weight
661 unfrozen s5.pathway1_res2.branch2.b_bn.weight
662 unfrozen s5.pathway1_res2.branch2.b_bn.bias
663 unfrozen s5.pathway1_res2.branch2.c.weight
664 unfrozen s5.pathway1_res2.branch2.c_bn.weight
665 unfrozen s5.pathway1_res2.branch2.c_bn.bias
666 unfrozen head.projection.weight
667 unfrozen head.projection.bias
[11/25 22:49:59][INFO] checkpoint.py: 507: Load from given checkpoint file.
[11/25 22:49:59][INFO] checkpoint.py: 214: Loading network weights from /srv/beegfs02/scratch/da_action/data/models_pretrained/SLOWFAST_32x2_R101_50_50.pkl.
[11/25 22:50:03][INFO] checkpoint.py: 340: Network weights head.projection.weight not loaded.
[11/25 22:50:03][INFO] checkpoint.py: 340: Network weights head.projection.bias not loaded.
[11/25 22:50:10][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/train.csv
[11/25 22:50:11][INFO] ava_helper.py: 111: Finished loading annotations from: /srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/ava_train_v2.2.csv
[11/25 22:50:11][INFO] ava_helper.py: 113: Detection threshold: 0.8
[11/25 22:50:11][INFO] ava_helper.py: 114: Number of unique boxes: 3847
[11/25 22:50:11][INFO] ava_helper.py: 115: Number of annotations: 5000
[11/25 22:50:11][INFO] ava_helper.py: 162: 2972 keyframes used.
[11/25 22:50:11][INFO] ava_dataset.py:  90: === AVA dataset summary ===
[11/25 22:50:11][INFO] ava_dataset.py:  91: Split: train
[11/25 22:50:11][INFO] ava_dataset.py:  92: Number of videos: 27
[11/25 22:50:11][INFO] ava_dataset.py:  96: Number of frames: 729814
[11/25 22:50:11][INFO] ava_dataset.py:  97: Number of key frames: 2972
[11/25 22:50:11][INFO] ava_dataset.py:  98: Number of boxes: 3847.
[11/25 22:50:13][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/val.csv
[11/25 22:50:13][INFO] ava_helper.py: 111: Finished loading annotations from: /srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/ava_val_predicted_boxes.csv
[11/25 22:50:13][INFO] ava_helper.py: 113: Detection threshold: 0.8
[11/25 22:50:13][INFO] ava_helper.py: 114: Number of unique boxes: 1218
[11/25 22:50:13][INFO] ava_helper.py: 115: Number of annotations: 0
[11/25 22:50:13][INFO] ava_helper.py: 162: 694 keyframes used.
[11/25 22:50:13][INFO] ava_dataset.py:  90: === AVA dataset summary ===
[11/25 22:50:13][INFO] ava_dataset.py:  91: Split: val
[11/25 22:50:13][INFO] ava_dataset.py:  92: Number of videos: 6
[11/25 22:50:13][INFO] ava_dataset.py:  96: Number of frames: 162182
[11/25 22:50:13][INFO] ava_dataset.py:  97: Number of key frames: 694
[11/25 22:50:13][INFO] ava_dataset.py:  98: Number of boxes: 1218.
[11/25 22:50:17][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/train.csv
[11/25 22:50:18][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/val.csv
[11/25 22:50:18][INFO] tensorboard_vis.py:  57: To see logged results in Tensorboard, please launch using the command             `tensorboard  --port=<port-number> --logdir /srv/beegfs02/scratch/da_action/data/output/ex_10_500_100_v4/tensorboard`
[11/25 22:50:18][INFO] train_net.py: 464: Start epoch: 2
[11/25 23:35:54][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/25 23:35:54][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/25 23:35:54][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/25 23:35:54][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/25 23:35:54][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/25 23:35:54][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.23878334821256506,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.11397762757965034,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5640228505482744,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.11810938880172643,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5599492952298813,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.286556827813449,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.6710788304419121,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.18851526136266739,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.34497721287888017,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12220505545337043,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3208175698322377}
[11/25 23:35:56][INFO] ava_eval_helper.py: 182: AVA eval done in 1.325116 seconds.
[11/25 23:35:56][INFO] logging.py:  97: json_stats: {
  "RAM": "13.82/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "2",
  "gpu_mem": "1.28G",
  "map": 0.32082,
  "mode": "val"
}
[11/26 00:21:27][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 00:21:27][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 00:21:27][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 00:21:27][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 00:21:27][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 00:21:27][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2643374807615814,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.16355061886025785,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5086588309044998,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.31925084552264815,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.4502545111121678,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.404159096537501,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.6864619607153332,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2731538727748422,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.7109099583699976,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.1695178212455719,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3950254996804401}
[11/26 00:21:28][INFO] ava_eval_helper.py: 182: AVA eval done in 1.439534 seconds.
[11/26 00:21:28][INFO] logging.py:  97: json_stats: {
  "RAM": "15.34/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "3",
  "gpu_mem": "1.28G",
  "map": 0.39503,
  "mode": "val"
}
[11/26 01:06:46][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 01:06:46][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 01:06:46][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 01:06:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 01:06:46][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 01:06:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.31188710942409925,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.24015694674668708,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.6156369591756439,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.27561081902122164,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5679764116873945,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.47848150690435953,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7521826020700804,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.28598597774916457,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.5756359635309443,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.14646307812070836,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4250017374430303}
[11/26 01:06:47][INFO] ava_eval_helper.py: 182: AVA eval done in 1.394315 seconds.
[11/26 01:06:47][INFO] logging.py:  97: json_stats: {
  "RAM": "12.60/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "4",
  "gpu_mem": "1.28G",
  "map": 0.42500,
  "mode": "val"
}
[11/26 01:52:07][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 01:52:07][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 01:52:07][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 01:52:07][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 01:52:07][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 01:52:07][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.27817236137233536,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2509001545497414,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5897895728935683,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.32507064845794986,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5718683473670424,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.48579225519211916,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7852026391342103,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2910595728698494,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.6198628777952621,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.14588068652475245,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4343599116156831}
[11/26 01:52:09][INFO] ava_eval_helper.py: 182: AVA eval done in 1.373430 seconds.
[11/26 01:52:09][INFO] logging.py:  97: json_stats: {
  "RAM": "12.75/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "5",
  "gpu_mem": "1.28G",
  "map": 0.43436,
  "mode": "val"
}
[11/26 02:36:22][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 02:36:22][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 02:36:22][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 02:36:22][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 02:36:22][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 02:36:22][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.23827783429102398,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.1993731893662965,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5299480254983254,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.36595004693288025,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5697311086414953,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4738660791436987,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.77802347780682,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2955675981403758,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.5222578992626857,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.16249915168443588,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4135494410768038}
[11/26 02:36:23][INFO] ava_eval_helper.py: 182: AVA eval done in 1.338749 seconds.
[11/26 02:36:23][INFO] logging.py:  97: json_stats: {
  "RAM": "12.53/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "6",
  "gpu_mem": "1.28G",
  "map": 0.41355,
  "mode": "val"
}
[11/26 03:19:50][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 03:19:50][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 03:19:50][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 03:19:50][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 03:19:50][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 03:19:50][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2784567115267954,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.16643397621230988,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5927537922030399,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.3480773853849811,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.640230422785714,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4822511235579553,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7288208399658874,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2261416932074608,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.5266046402321307,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.15615458571820112,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.41459251707944755}
[11/26 03:19:51][INFO] ava_eval_helper.py: 182: AVA eval done in 1.466646 seconds.
[11/26 03:19:51][INFO] logging.py:  97: json_stats: {
  "RAM": "12.75/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "7",
  "gpu_mem": "1.28G",
  "map": 0.41459,
  "mode": "val"
}
[11/26 04:03:28][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 04:03:28][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 04:03:28][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 04:03:28][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 04:03:29][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 04:03:29][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.24724390811263025,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2594474805742473,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5673804928668499,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.380639702921836,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5581344273406069,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.42878264778888825,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7412963777776735,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.29437631127017727,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.5408611751354772,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.16258616823442057,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.41807486920228076}
[11/26 04:03:30][INFO] ava_eval_helper.py: 182: AVA eval done in 1.368709 seconds.
[11/26 04:03:30][INFO] logging.py:  97: json_stats: {
  "RAM": "12.53/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "8",
  "gpu_mem": "1.28G",
  "map": 0.41807,
  "mode": "val"
}
[11/26 04:47:27][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 04:47:27][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 04:47:27][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 04:47:27][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 04:47:27][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 04:47:27][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2655093614041887,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2268078794319982,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5747430888700249,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4069978268673058,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.45361398337869885,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.5008329675959311,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7721092987011386,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2190567986302312,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.582827646063089,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11890634725118134,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.41214051981937877}
[11/26 04:47:28][INFO] ava_eval_helper.py: 182: AVA eval done in 1.466334 seconds.
[11/26 04:47:28][INFO] logging.py:  97: json_stats: {
  "RAM": "12.54/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "9",
  "gpu_mem": "1.28G",
  "map": 0.41214,
  "mode": "val"
}
[11/26 05:31:46][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 05:31:46][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 05:31:46][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 05:31:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 05:31:46][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 05:31:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.24190992363674113,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.21559310997002032,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.6421353463236872,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4061104927024249,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5608678319863777,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4944530540780817,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7144995192169763,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2805549474676055,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.45114929563118533,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.10235120415101909,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.41096247251641194}
[11/26 05:31:47][INFO] ava_eval_helper.py: 182: AVA eval done in 1.334310 seconds.
[11/26 05:31:47][INFO] logging.py:  97: json_stats: {
  "RAM": "13.27/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "10",
  "gpu_mem": "1.28G",
  "map": 0.41096,
  "mode": "val"
}
[11/26 06:16:04][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 06:16:04][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 06:16:04][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 06:16:04][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 06:16:04][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 06:16:04][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2680480032961032,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.14081889558330524,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.6182589668555798,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.45522031144345376,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5634776130371094,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4096179637295101,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.734850826974058,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2625449838773625,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.48741068150117567,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11410113359401598,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4054349379891674}
[11/26 06:16:06][INFO] ava_eval_helper.py: 182: AVA eval done in 1.305811 seconds.
[11/26 06:16:06][INFO] logging.py:  97: json_stats: {
  "RAM": "12.49/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "11",
  "gpu_mem": "1.28G",
  "map": 0.40543,
  "mode": "val"
}
[11/26 07:00:36][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 07:00:36][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 07:00:36][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 07:00:36][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 07:00:36][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 07:00:36][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2372016908969576,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.20562671144939895,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5812635442304759,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4536626810215153,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5371105208358169,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.44149261791373373,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7683800988375543,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2890917757166103,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.33515705117124117,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11451163376770677,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3963498325841011}
[11/26 07:00:37][INFO] ava_eval_helper.py: 182: AVA eval done in 1.420369 seconds.
[11/26 07:00:37][INFO] logging.py:  97: json_stats: {
  "RAM": "12.87/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "12",
  "gpu_mem": "1.28G",
  "map": 0.39635,
  "mode": "val"
}
[11/26 07:44:28][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 07:44:28][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 07:44:28][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 07:44:28][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 07:44:28][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 07:44:28][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2413509966545466,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.18945732930611145,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5155900816405696,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.41532154933140963,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.4862291044305134,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.44438081811728997,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7949333512211405,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2933117734665569,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.41893489442380444,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.14871533013098542,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3948225228722928}
[11/26 07:44:29][INFO] ava_eval_helper.py: 182: AVA eval done in 1.326375 seconds.
[11/26 07:44:29][INFO] logging.py:  97: json_stats: {
  "RAM": "12.58/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "13",
  "gpu_mem": "1.28G",
  "map": 0.39482,
  "mode": "val"
}
[11/26 08:28:34][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 08:28:34][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 08:28:34][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 08:28:34][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 08:28:34][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 08:28:34][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2524266432617795,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2450005599193876,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5686204892057618,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4384761463844778,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5463665573996693,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.367948326149398,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7703379372143093,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.324948534111018,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.4366013598612034,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.15016930726655286,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4100895860773558}
[11/26 08:28:35][INFO] ava_eval_helper.py: 182: AVA eval done in 1.427412 seconds.
[11/26 08:28:35][INFO] logging.py:  97: json_stats: {
  "RAM": "12.86/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "14",
  "gpu_mem": "1.28G",
  "map": 0.41009,
  "mode": "val"
}
[11/26 09:13:17][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 09:13:17][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 09:13:17][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 09:13:17][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 09:13:17][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 09:13:17][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.24180586508995197,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.22157390841158495,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5913602502416502,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4509156994709288,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5305882601892491,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.39257791703320916,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7591278998992836,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.25607075224569115,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.42047953069374794,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.14302633063720882,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.40075264139125066}
[11/26 09:13:18][INFO] ava_eval_helper.py: 182: AVA eval done in 1.380927 seconds.
[11/26 09:13:18][INFO] logging.py:  97: json_stats: {
  "RAM": "12.69/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "15",
  "gpu_mem": "1.28G",
  "map": 0.40075,
  "mode": "val"
}
[11/26 09:57:56][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 09:57:56][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 09:57:56][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 09:57:56][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 09:57:56][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 09:57:56][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.24252666553265856,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2004583926155035,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5709132289021397,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.40268972485316556,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.575158416007347,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.39287524250780154,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7131281687067842,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2649069594901022,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.4945219662920674,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.13229860066807703,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3989477365575646}
[11/26 09:57:57][INFO] ava_eval_helper.py: 182: AVA eval done in 1.336202 seconds.
[11/26 09:57:57][INFO] logging.py:  97: json_stats: {
  "RAM": "10.64/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "16",
  "gpu_mem": "1.28G",
  "map": 0.39895,
  "mode": "val"
}
[11/26 10:45:34][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 10:45:34][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 10:45:34][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 10:45:34][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 10:45:34][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 10:45:34][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22325311937851375,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.1928604806031723,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5628977655033558,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.49178140618952826,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5010839804725508,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.41199260008136923,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.6982302122042223,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.28328001526378976,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.47027856396772993,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12335855994746645,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.39590167036116986}
[11/26 10:45:35][INFO] ava_eval_helper.py: 182: AVA eval done in 1.344221 seconds.
[11/26 10:45:35][INFO] logging.py:  97: json_stats: {
  "RAM": "10.38/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "17",
  "gpu_mem": "1.28G",
  "map": 0.39590,
  "mode": "val"
}
[11/26 11:32:49][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 11:32:49][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 11:32:49][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 11:32:49][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 11:32:49][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 11:32:49][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2186010196311363,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.19802284916686486,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5678728773209453,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.44456636651649184,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5446380913575859,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.3577098988320343,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7525053135816889,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.27877170584680055,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3918853030219158,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.1446727238476896,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.38992461491231534}
[11/26 11:32:51][INFO] ava_eval_helper.py: 182: AVA eval done in 1.349052 seconds.
[11/26 11:32:51][INFO] logging.py:  97: json_stats: {
  "RAM": "10.34/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "18",
  "gpu_mem": "1.28G",
  "map": 0.38992,
  "mode": "val"
}
[11/26 12:21:52][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 12:21:52][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 12:21:52][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 12:21:52][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 12:21:52][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 12:21:52][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2422874430627397,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.17642584745191486,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5884694730967497,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4375418482553834,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5220476648030431,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4081242282101416,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7085077052600692,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.26039259404983806,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.35692525455771923,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.14654400120204286,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3847266059949642}
[11/26 12:21:54][INFO] ava_eval_helper.py: 182: AVA eval done in 1.475278 seconds.
[11/26 12:21:54][INFO] logging.py:  97: json_stats: {
  "RAM": "16.71/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "19",
  "gpu_mem": "1.28G",
  "map": 0.38473,
  "mode": "val"
}
[11/26 13:07:47][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 13:07:47][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 13:07:47][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 13:07:47][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 13:07:47][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 13:07:47][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.27589909452991956,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2148879156190061,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5718742960524896,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.43847145527944253,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5894541689504669,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4223202578552125,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7154415420571716,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2538756686459827,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.37316333254608963,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.1499658528618916,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4005353584397673}
[11/26 13:07:48][INFO] ava_eval_helper.py: 182: AVA eval done in 1.381696 seconds.
[11/26 13:07:48][INFO] logging.py:  97: json_stats: {
  "RAM": "17.23/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "20",
  "gpu_mem": "1.28G",
  "map": 0.40054,
  "mode": "val"
}
[11/26 13:54:23][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 13:54:23][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 13:54:23][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 13:54:23][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 13:54:23][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 13:54:23][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2201899168032402,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.21242015638612446,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5849950256690266,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.43313779240341244,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.585613123143368,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.3567080216007865,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.6945967855458463,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2843033217134154,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3252847642099086,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11554310393151446,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3812792011406643}
[11/26 13:54:24][INFO] ava_eval_helper.py: 182: AVA eval done in 1.416050 seconds.
[11/26 13:54:24][INFO] logging.py:  97: json_stats: {
  "RAM": "18.55/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "21",
  "gpu_mem": "1.28G",
  "map": 0.38128,
  "mode": "val"
}
[11/26 14:40:46][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 14:40:46][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 14:40:46][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 14:40:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 14:40:46][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 14:40:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.23162091305929852,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.22851723179442052,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5859157926402104,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.47929200658096466,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5533173449908177,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.43006750781071806,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.736424366743773,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.23250157844572314,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3116198949517114,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.15950169642102174,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3948778333438659}
[11/26 14:40:48][INFO] ava_eval_helper.py: 182: AVA eval done in 1.502107 seconds.
[11/26 14:40:48][INFO] logging.py:  97: json_stats: {
  "RAM": "19.50/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "22",
  "gpu_mem": "1.28G",
  "map": 0.39488,
  "mode": "val"
}
[11/26 15:27:12][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 15:27:12][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 15:27:12][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 15:27:12][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 15:27:12][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 15:27:12][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.24456269559073998,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.263946026320097,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5612965402766756,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4544526864743556,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5485942297429891,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.41579253578331554,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7352966598201461,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.26268337168854017,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.311942796931906,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12595730533005736,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.39245248479588224}
[11/26 15:27:14][INFO] ava_eval_helper.py: 182: AVA eval done in 1.363239 seconds.
[11/26 15:27:14][INFO] logging.py:  97: json_stats: {
  "RAM": "7.62/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "23",
  "gpu_mem": "1.28G",
  "map": 0.39245,
  "mode": "val"
}
[11/26 16:15:40][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 16:15:40][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 16:15:40][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 16:15:40][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 16:15:40][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 16:15:40][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2354510146004887,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.274729974370085,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5722045146656942,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.46102659661958945,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5629664334526047,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.48499017952483997,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7570141042065662,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2722575664002656,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.2835788393904792,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.13231802153567987,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4036537244766293}
[11/26 16:15:42][INFO] ava_eval_helper.py: 182: AVA eval done in 1.640235 seconds.
[11/26 16:15:42][INFO] logging.py:  97: json_stats: {
  "RAM": "7.67/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "24",
  "gpu_mem": "1.28G",
  "map": 0.40365,
  "mode": "val"
}
[11/26 17:03:46][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 17:03:46][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 17:03:46][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 17:03:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 17:03:46][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 17:03:46][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22195552665646287,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.28272888193898527,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.573027964327701,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.48025833350794656,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5744327089161528,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.42848869569965864,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7142387909982522,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.25417299505164975,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.2791236919004992,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11918386709550302,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3927611456092811}
[11/26 17:03:47][INFO] ava_eval_helper.py: 182: AVA eval done in 1.363333 seconds.
[11/26 17:03:47][INFO] logging.py:  97: json_stats: {
  "RAM": "7.62/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "25",
  "gpu_mem": "1.28G",
  "map": 0.39276,
  "mode": "val"
}
[11/26 17:51:32][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 17:51:32][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 17:51:32][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 17:51:32][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 17:51:32][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 17:51:32][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.23735040952405867,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.29556476296893436,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5980289017812426,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4842850183017558,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5907638952262639,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4496314832031669,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7289931011714733,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.23226007668461593,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.307272598180752,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11555868903217228,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4039708936074436}
[11/26 17:51:34][INFO] ava_eval_helper.py: 182: AVA eval done in 1.554342 seconds.
[11/26 17:51:34][INFO] logging.py:  97: json_stats: {
  "RAM": "7.65/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "26",
  "gpu_mem": "1.28G",
  "map": 0.40397,
  "mode": "val"
}
[11/26 18:39:27][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 18:39:27][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 18:39:27][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 18:39:27][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 18:39:27][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 18:39:27][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22283958669156842,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2874287953398488,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5609176645202932,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.45158870294480014,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5448266840281212,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.43827135899249176,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7265381067541019,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2442805523118446,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.2751909187757829,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.1505445852123371,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.39024269555711905}
[11/26 18:39:28][INFO] ava_eval_helper.py: 182: AVA eval done in 1.497841 seconds.
[11/26 18:39:28][INFO] logging.py:  97: json_stats: {
  "RAM": "7.67/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "27",
  "gpu_mem": "1.28G",
  "map": 0.39024,
  "mode": "val"
}
[11/26 19:27:01][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 19:27:01][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 19:27:01][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 19:27:01][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 19:27:01][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 19:27:01][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.239133997959936,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2839637296906097,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5816096980810619,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.48400725839440456,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5333083970187468,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.47770539313547317,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7222932897985495,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2452593073290393,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3209581088838889,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12895177900983648,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.4017190959301546}
[11/26 19:27:02][INFO] ava_eval_helper.py: 182: AVA eval done in 1.416739 seconds.
[11/26 19:27:02][INFO] logging.py:  97: json_stats: {
  "RAM": "17.90/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "28",
  "gpu_mem": "1.28G",
  "map": 0.40172,
  "mode": "val"
}
[11/26 20:14:29][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 20:14:29][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 20:14:29][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 20:14:29][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 20:14:29][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 20:14:29][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.21209522575771733,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.31174186871828574,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.548665827382248,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4285524271153357,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5475569977574721,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.42971158205862947,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.6819130870016041,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2323959094663912,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.32270126595678744,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.13418790989203885,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.384952210110651}
[11/26 20:14:30][INFO] ava_eval_helper.py: 182: AVA eval done in 1.442767 seconds.
[11/26 20:14:30][INFO] logging.py:  97: json_stats: {
  "RAM": "21.10/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "29",
  "gpu_mem": "1.28G",
  "map": 0.38495,
  "mode": "val"
}
[11/26 21:01:29][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 21:01:29][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 21:01:29][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 21:01:29][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 21:01:29][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 21:01:29][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22484599927839202,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.283712568831584,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5790415814386505,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.45399802967526315,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5695132038674094,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.49265500002026574,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7341506500207852,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.22647112012895115,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.29412730945599236,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12794138257341525,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.39864568452907084}
[11/26 21:01:31][INFO] ava_eval_helper.py: 182: AVA eval done in 1.416823 seconds.
[11/26 21:01:31][INFO] logging.py:  97: json_stats: {
  "RAM": "21.46/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "30",
  "gpu_mem": "1.28G",
  "map": 0.39865,
  "mode": "val"
}
[11/26 21:48:42][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 21:48:42][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 21:48:42][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 21:48:42][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 21:48:42][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 21:48:42][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.21124396924431882,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.29679399415770275,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.565266895626028,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4750561754629434,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5790681503424149,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.44298773440462197,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7153918023277004,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.23509646162997644,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3413191785995593,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.1311104410226865,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3993334802817952}
[11/26 21:48:44][INFO] ava_eval_helper.py: 182: AVA eval done in 1.716615 seconds.
[11/26 21:48:44][INFO] logging.py:  97: json_stats: {
  "RAM": "17.96/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "31",
  "gpu_mem": "1.28G",
  "map": 0.39933,
  "mode": "val"
}
[11/26 22:33:18][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 22:33:18][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 22:33:18][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 22:33:18][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 22:33:18][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 22:33:18][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22071872157652223,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.27618895321571196,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5472997344829357,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.41964394088615997,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.539695095195067,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4253487335034961,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7139721389539692,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2333176020201954,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3390670844986291,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12976320676068642,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.38450152110933733}
[11/26 22:33:20][INFO] ava_eval_helper.py: 182: AVA eval done in 1.438058 seconds.
[11/26 22:33:20][INFO] logging.py:  97: json_stats: {
  "RAM": "22.70/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "32",
  "gpu_mem": "1.28G",
  "map": 0.38450,
  "mode": "val"
}
[11/26 23:23:41][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/26 23:23:41][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/26 23:23:41][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/26 23:23:41][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/26 23:23:41][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/26 23:23:41][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22042094424712988,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2911581901056496,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5719335296247625,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4201583231922339,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5497854990775651,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4132626276054758,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7058621472922466,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.24784708265720515,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.30061367322353255,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11695058039216856,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.383799259741797}
[11/26 23:23:42][INFO] ava_eval_helper.py: 182: AVA eval done in 1.401491 seconds.
[11/26 23:23:42][INFO] logging.py:  97: json_stats: {
  "RAM": "18.28/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "33",
  "gpu_mem": "1.28G",
  "map": 0.38380,
  "mode": "val"
}
[11/27 00:19:23][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/27 00:19:23][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/27 00:19:23][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/27 00:19:23][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/27 00:19:23][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/27 00:19:23][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.21703150987747644,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.28951600239451886,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5566940586265943,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4206198317900981,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.550517953811124,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.44858513014841855,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7369648309405785,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.22954387661508602,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.33628366499640966,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11997421454137214,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.39057310737416767}
[11/27 00:19:24][INFO] ava_eval_helper.py: 182: AVA eval done in 1.554641 seconds.
[11/27 00:19:24][INFO] logging.py:  97: json_stats: {
  "RAM": "22.79/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "34",
  "gpu_mem": "1.28G",
  "map": 0.39057,
  "mode": "val"
}
[11/27 01:15:47][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/27 01:15:47][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/27 01:15:47][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/27 01:15:47][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/27 01:15:47][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/27 01:15:47][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22395988427568936,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2887364522240489,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.565238064986614,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4348886431128075,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5448951913972713,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4247339718682999,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7170399601279368,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.24929533927166658,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3169496847711169,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12072208154562045,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3886459273581072}
[11/27 01:15:48][INFO] ava_eval_helper.py: 182: AVA eval done in 1.469744 seconds.
[11/27 01:15:48][INFO] logging.py:  97: json_stats: {
  "RAM": "18.31/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "35",
  "gpu_mem": "1.28G",
  "map": 0.38865,
  "mode": "val"
}
[11/27 02:12:48][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/27 02:12:48][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/27 02:12:48][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/27 02:12:48][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/27 02:12:48][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/27 02:12:48][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22950592107485737,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.286631444696386,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5693333546127373,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4375492418259707,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5409753897274423,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4461740840385861,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7080415996206566,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.241168882553533,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.29524626209533433,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11526795377622184,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3869894134021726}
[11/27 02:12:49][INFO] ava_eval_helper.py: 182: AVA eval done in 1.411211 seconds.
[11/27 02:12:49][INFO] logging.py:  97: json_stats: {
  "RAM": "7.64/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "36",
  "gpu_mem": "1.28G",
  "map": 0.38699,
  "mode": "val"
}
[11/27 03:10:39][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/27 03:10:39][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/27 03:10:39][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/27 03:10:39][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/27 03:10:39][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/27 03:10:39][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2295822739187637,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2825656047748783,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.563199315677076,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4429523093154003,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5449740134391305,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4247288253050128,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7188661308401624,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.23900342625091198,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.31303853619960487,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.1361899433550362,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.38951003790759775}
[11/27 03:10:40][INFO] ava_eval_helper.py: 182: AVA eval done in 1.479352 seconds.
[11/27 03:10:40][INFO] logging.py:  97: json_stats: {
  "RAM": "7.64/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "37",
  "gpu_mem": "1.28G",
  "map": 0.38951,
  "mode": "val"
}
[11/27 04:08:38][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/27 04:08:38][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/27 04:08:38][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/27 04:08:38][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/27 04:08:38][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/27 04:08:38][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22454951770947099,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.31149679964631866,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5586686938283106,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.44122621938052653,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5589504731504765,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.43515331741362245,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7248191504164597,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2365647140954706,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.3057397428771522,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12494761325281434,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.39221162417706223}
[11/27 04:08:40][INFO] ava_eval_helper.py: 182: AVA eval done in 1.519733 seconds.
[11/27 04:08:40][INFO] logging.py:  97: json_stats: {
  "RAM": "7.61/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "38",
  "gpu_mem": "1.28G",
  "map": 0.39221,
  "mode": "val"
}
[11/27 05:06:09][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/27 05:06:09][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/27 05:06:09][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/27 05:06:09][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/27 05:06:09][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/27 05:06:09][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2210271294201373,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.2866034212479608,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5654806876662375,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.44393235626962346,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5372250852980737,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4505310542685583,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7104298437902545,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.2514877726375124,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.30816132404158403,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11863533750842334,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.3893514012148366}
[11/27 05:06:10][INFO] ava_eval_helper.py: 182: AVA eval done in 1.316404 seconds.
[11/27 05:06:10][INFO] logging.py:  97: json_stats: {
  "RAM": "7.63/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "39",
  "gpu_mem": "1.28G",
  "map": 0.38935,
  "mode": "val"
}
[11/27 06:02:38][INFO] ava_eval_helper.py: 165: Evaluating with 700 unique GT frames.
[11/27 06:02:38][INFO] ava_eval_helper.py: 167: Evaluating with 694 unique detection frames
[11/27 06:02:38][INFO] ava_eval_helper.py: 322: AVA results wrote to detections_latest.csv
[11/27 06:02:38][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[11/27 06:02:38][INFO] ava_eval_helper.py: 322: AVA results wrote to groundtruth_latest.csv
[11/27 06:02:38][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.22495947451765558,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.28565337117595824,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5856460936677461,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4578257306733274,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5533844417296009,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.45340401604573843,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.7021448479945749,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.23538883381575623,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.32273419705979806,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.12476548136826043,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.39459064880484157}
[11/27 06:02:39][INFO] ava_eval_helper.py: 182: AVA eval done in 1.387652 seconds.
[11/27 06:02:39][INFO] logging.py:  97: json_stats: {
  "RAM": "7.69/251.80G",
  "_type": "val_epoch",
  "cur_epoch": "40",
  "gpu_mem": "1.28G",
  "map": 0.39459,
  "mode": "val"
}
[11/27 06:02:39][INFO] test_net.py: 160: Test with config:
[11/27 06:02:39][INFO] test_net.py: 161: AVA:
  ANNOTATION_DIR: /srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/
  BGR: False
  DETECTION_SCORE_THRESH: 0.8
  EXCLUSION_FILE: ava_val_excluded_timestamps_v2.2.csv
  FRAME_DIR: /srv/beegfs02/scratch/da_action/data/ava/frames/
  FRAME_LIST_DIR: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/
  FULL_TEST_ON_VAL: True
  GROUNDTRUTH_FILE: ava_val_v2.2.csv
  IMG_PROC_BACKEND: cv2
  LABEL_MAP_FILE: ava_action_list_v2.2.pbtxt
  TEST_FORCE_FLIP: False
  TEST_LISTS: ['val.csv']
  TEST_PREDICT_BOX_LISTS: ['ava_val_predicted_boxes.csv']
  TRAIN_GT_BOX_LISTS: ['ava_train_v2.2.csv']
  TRAIN_LISTS: ['train.csv']
  TRAIN_PCA_EIGVAL: [0.225, 0.224, 0.229]
  TRAIN_PCA_EIGVEC: [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]]
  TRAIN_PCA_JITTER_ONLY: True
  TRAIN_PREDICT_BOX_LISTS: []
  TRAIN_USE_COLOR_AUGMENTATION: False
BENCHMARK:
  LOG_PERIOD: 100
  NUM_EPOCHS: 5
  SHUFFLE: True
BN:
  NORM_TYPE: batchnorm
  NUM_BATCHES_PRECISE: 200
  NUM_SPLITS: 1
  NUM_SYNC_DEVICES: 1
  USE_PRECISE_STATS: False
  WEIGHT_DECAY: 0.0
DATA:
  DECODING_BACKEND: pyav
  ENSEMBLE_METHOD: sum
  INPUT_CHANNEL_NUM: [3, 3]
  INV_UNIFORM_SAMPLE: False
  MEAN: [0.45, 0.45, 0.45]
  MULTI_LABEL: False
  NUM_FRAMES: 32
  PATH_LABEL_SEPARATOR:  
  PATH_PREFIX: 
  PATH_TO_DATA_DIR: 
  RANDOM_FLIP: True
  REVERSE_INPUT_CHANNEL: False
  SAMPLING_RATE: 2
  STD: [0.225, 0.225, 0.225]
  TARGET_FPS: 30
  TEST_CROP_SIZE: 256
  TRAIN_CROP_SIZE: 224
  TRAIN_JITTER_SCALES: [256, 320]
DATA_LOADER:
  ENABLE_MULTI_THREAD_DECODE: False
  NUM_WORKERS: 1
  PIN_MEMORY: True
DEMO:
  BUFFER_SIZE: 0
  CLIP_VIS_SIZE: 10
  COMMON_CLASS_NAMES: ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)']
  COMMON_CLASS_THRES: 0.7
  DETECTRON2_CFG: COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml
  DETECTRON2_THRESH: 0.9
  DETECTRON2_WEIGHTS: detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl
  DISPLAY_HEIGHT: 0
  DISPLAY_WIDTH: 0
  ENABLE: False
  FPS: 30
  GT_BOXES: 
  INPUT_FORMAT: BGR
  INPUT_VIDEO: 
  LABEL_FILE_PATH: 
  NUM_CLIPS_SKIP: 0
  NUM_VIS_INSTANCES: 2
  OUTPUT_FILE: 
  OUTPUT_FPS: -1
  PREDS_BOXES: 
  SLOWMO: 1
  STARTING_SECOND: 900
  THREAD_ENABLE: False
  UNCOMMON_CLASS_THRES: 0.3
  VIS_MODE: thres
  WEBCAM: -1
DETECTION:
  ALIGNED: False
  ENABLE: True
  ROI_XFORM_RESOLUTION: 7
  SPATIAL_SCALE_FACTOR: 16
DIST_BACKEND: nccl
LOG_MODEL_INFO: False
LOG_PERIOD: 10
MODEL:
  ARCH: slowfast
  DROPCONNECT_RATE: 0.0
  DROPOUT_RATE: 0.5
  FC_INIT_STD: 0.01
  FREEZE_TO: 605
  HEAD_ACT: sigmoid
  LOSS_FUNC: bce
  MODEL_NAME: SlowFast
  MULTI_PATHWAY_ARCH: ['slowfast']
  NUM_CLASSES: 10
  SINGLE_PATHWAY_ARCH: ['c2d', 'i3d', 'slow', 'x3d']
MULTIGRID:
  BN_BASE_SIZE: 8
  DEFAULT_B: 0
  DEFAULT_S: 0
  DEFAULT_T: 0
  EPOCH_FACTOR: 1.5
  EVAL_FREQ: 3
  LONG_CYCLE: False
  LONG_CYCLE_FACTORS: [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)]
  LONG_CYCLE_SAMPLING_RATE: 0
  SHORT_CYCLE: False
  SHORT_CYCLE_FACTORS: [0.5, 0.7071067811865476]
NONLOCAL:
  GROUP: [[1, 1], [1, 1], [1, 1], [1, 1]]
  INSTANTIATION: dot_product
  LOCATION: [[[], []], [[], []], [[6, 13, 20], []], [[], []]]
  POOL: [[[2, 2, 2], [2, 2, 2]], [[2, 2, 2], [2, 2, 2]], [[2, 2, 2], [2, 2, 2]], [[2, 2, 2], [2, 2, 2]]]
NUM_GPUS: 1
NUM_SHARDS: 1
OUTPUT_DIR: /srv/beegfs02/scratch/da_action/data/output/ex_10_500_100_v4
RESNET:
  DEPTH: 101
  INPLACE_RELU: True
  NUM_BLOCK_TEMP_KERNEL: [[3, 3], [4, 4], [6, 6], [3, 3]]
  NUM_GROUPS: 1
  SPATIAL_DILATIONS: [[1, 1], [1, 1], [1, 1], [2, 2]]
  SPATIAL_STRIDES: [[1, 1], [2, 2], [2, 2], [1, 1]]
  STRIDE_1X1: False
  TRANS_FUNC: bottleneck_transform
  WIDTH_PER_GROUP: 64
  ZERO_INIT_FINAL_BN: True
RNG_SEED: 0
SHARD_ID: 0
SLOWFAST:
  ALPHA: 4
  BETA_INV: 8
  FUSION_CONV_CHANNEL_RATIO: 2
  FUSION_KERNEL_SZ: 5
SOLVER:
  BASE_LR: 0.1
  BASE_LR_SCALE_NUM_SHARDS: False
  COSINE_END_LR: 0.0
  DAMPENING: 0.0
  GAMMA: 0.1
  LRS: []
  LR_POLICY: cosine
  MAX_EPOCH: 40
  MOMENTUM: 0.9
  NESTEROV: True
  OPTIMIZING_METHOD: sgd
  STEPS: []
  STEP_SIZE: 1
  WARMUP_EPOCHS: 0.0
  WARMUP_FACTOR: 0.1
  WARMUP_START_LR: 0.01
  WEIGHT_DECAY: 1e-07
TENSORBOARD:
  CATEGORIES_PATH: 
  CLASS_NAMES_PATH: 
  CONFUSION_MATRIX:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
  ENABLE: True
  HISTOGRAM:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
    TOPK: 10
  LOG_DIR: tensorboard
  MODEL_VIS:
    ACTIVATIONS: False
    COLORMAP: Pastel2
    ENABLE: False
    GRAD_CAM:
      COLORMAP: viridis
      ENABLE: True
      LAYER_LIST: []
      USE_TRUE_LABEL: False
    INPUT_VIDEO: False
    LAYER_LIST: []
    MODEL_WEIGHTS: False
    TOPK_PREDS: 1
  PREDICTIONS_PATH: 
  WRONG_PRED_VIS:
    ENABLE: False
    SUBSET_PATH: 
    TAG: Incorrectly classified videos.
TEST:
  BATCH_SIZE: 4
  CHECKPOINT_FILE_PATH: 
  CHECKPOINT_TYPE: pytorch
  DATASET: ava
  DETECTIONS_PATH: 
  ENABLE: True
  GT_PATH: 
  NUM_ENSEMBLE_VIEWS: 10
  NUM_SPATIAL_CROPS: 3
  SAVE_RESULTS_PATH: 
TRAIN:
  AUTO_RESUME: False
  BATCH_SIZE: 4
  CHECKPOINT_CLEAR_NAME_PATTERN: ()
  CHECKPOINT_EPOCH_RESET: False
  CHECKPOINT_FILE_PATH: /srv/beegfs02/scratch/da_action/data/models_pretrained/SLOWFAST_32x2_R101_50_50.pkl
  CHECKPOINT_INFLATE: False
  CHECKPOINT_PERIOD: 1
  CHECKPOINT_TYPE: pytorch
  DATASET: ava
  ENABLE: True
  EVAL_PERIOD: 1
X3D:
  BN_LIN5: False
  BOTTLENECK_FACTOR: 1.0
  CHANNELWISE_3x3x3: True
  DEPTH_FACTOR: 1.0
  DIM_C1: 12
  DIM_C5: 2048
  SCALE_RES2: False
  WIDTH_FACTOR: 1.0
[11/27 06:02:40][INFO] checkpoint.py: 214: Loading network weights from /srv/beegfs02/scratch/da_action/data/output/ex_10_500_100_v4/checkpoints/checkpoint_epoch_00040.pyth.
[11/27 06:02:46][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/val.csv
[11/27 06:02:46][INFO] ava_helper.py: 111: Finished loading annotations from: /srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/ava_val_predicted_boxes.csv
[11/27 06:02:46][INFO] ava_helper.py: 113: Detection threshold: 0.8
[11/27 06:02:46][INFO] ava_helper.py: 114: Number of unique boxes: 1218
[11/27 06:02:46][INFO] ava_helper.py: 115: Number of annotations: 0
[11/27 06:02:46][INFO] ava_helper.py: 162: 694 keyframes used.
[11/27 06:02:46][INFO] ava_dataset.py:  90: === AVA dataset summary ===
[11/27 06:02:46][INFO] ava_dataset.py:  91: Split: test
[11/27 06:02:46][INFO] ava_dataset.py:  92: Number of videos: 6
[11/27 06:02:46][INFO] ava_dataset.py:  96: Number of frames: 162182
[11/27 06:02:46][INFO] ava_dataset.py:  97: Number of key frames: 694
[11/27 06:02:46][INFO] ava_dataset.py:  98: Number of boxes: 1218.
[11/27 06:02:46][INFO] test_net.py: 172: Testing model for 174 iterations
Traceback (most recent call last):
  File "tools/run_net.py", line 53, in <module>
    main()
  File "tools/run_net.py", line 37, in main
    launch_job(cfg=cfg, init_method=args.init_method, func=test)
  File "/home/sieberl/SA2020/pyslowfast/slowfast/slowfast/utils/misc.py", line 296, in launch_job
    func(cfg=cfg)
  File "/home/sieberl/SA2020/pyslowfast/slowfast/tools/test_net.py", line 175, in test
    assert cfg.NUM_GPUS == cfg.TEST.BATCH_SIZE or cfg.NUM_GPUS == 0
AssertionError
