
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


[12/30 21:46:42][INFO] test_net.py: 177: Test with config:
[12/30 21:46:42][INFO] test_net.py: 178: AVA:
  ANNOTATION_DIR: /srv/beegfs02/scratch/da_action/data/kinetics700/80_annotations_10_500_100/
  BGR: False
  DETECTION_SCORE_THRESH: 0.8
  EXCLUSION_FILE: kinetics_val_excluded_timestamps_v2.1.csv
  FRAME_DIR: /srv/beegfs02/scratch/da_action/data/kinetics700/frames/
  FRAME_LIST_DIR: /srv/beegfs02/scratch/da_action/data/kinetics700/80_frame_lists_10_500_100/
  FULL_TEST_ON_VAL: True
  GROUNDTRUTH_FILE: kinetics_val_v2.1.csv
  IMG_PROC_BACKEND: cv2
  LABEL_MAP_FILE: ava_action_list_v2.2.pbtxt
  TEST_FORCE_FLIP: False
  TEST_LISTS: ['val.csv']
  TEST_PREDICT_BOX_LISTS: ['kinetics_val_predicted_boxes.csv']
  TRAIN_GT_BOX_LISTS: ['ava_train_v2.2.csv']
  TRAIN_LISTS: ['train.csv']
  TRAIN_PCA_EIGVAL: [0.225, 0.224, 0.229]
  TRAIN_PCA_EIGVEC: [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]]
  TRAIN_PCA_JITTER_ONLY: True
  TRAIN_PREDICT_BOX_LISTS: []
  TRAIN_USE_COLOR_AUGMENTATION: False
BENCHMARK:
  LOG_PERIOD: 100
  NUM_EPOCHS: 5
  SHUFFLE: True
BN:
  NORM_TYPE: batchnorm
  NUM_BATCHES_PRECISE: 200
  NUM_SPLITS: 1
  NUM_SYNC_DEVICES: 1
  USE_PRECISE_STATS: False
  WEIGHT_DECAY: 0.0
DA:
  ANNOTATION_DIR: /srv/beegfs02/scratch/da_action/data/kinetics700/annotations_10_5_2/
  AUX: False
  AUX_TEST: False
  CLASSES: 0
  DATASET: da
  DETECTION_SCORE_THRESH: 0.8
  ENABLE: False
  EXCLUSION_FILE: kinetics_val_excluded_timestamps_v2.1.csv
  FRAME_DIR: /srv/beegfs02/scratch/da_action/data/kinetics700/frames/
  FRAME_LIST_DIR: /srv/beegfs02/scratch/da_action/data/kinetics700/frame_lists_10_5_2/
  FULL_TEST_ON_VAL: True
  F_CONFUSION: -1
  GROUNDTRUTH_FILE: kinetics_val_v2.1.csv
  LABEL_MAP_FILE: ava_action_list_v2.2.pbtxt
  LR_FACTOR: 10.0
  TEST_GT_BOX_LISTS: []
  TEST_LISTS: ['val.csv']
  TEST_PREDICT_BOX_LISTS: ['kinetics_val_predicted_boxes.csv']
  TRAIN_GT_BOX_LISTS: []
  TRAIN_LISTS: ['train.csv']
  TRAIN_PREDICT_BOX_LISTS: ['kinetics_train_predicted_boxes.csv']
  WEIGHT_AUX: 0.5
  WEIGHT_MAIN: 0.5
DATA:
  DECODING_BACKEND: pyav
  ENSEMBLE_METHOD: sum
  INPUT_CHANNEL_NUM: [3, 3]
  INV_UNIFORM_SAMPLE: False
  MEAN: [0.45, 0.45, 0.45]
  MULTI_LABEL: False
  NUM_FRAMES: 32
  PATH_LABEL_SEPARATOR:  
  PATH_PREFIX: 
  PATH_TO_DATA_DIR: 
  RANDOM_FLIP: True
  REVERSE_INPUT_CHANNEL: False
  SAMPLING_RATE: 2
  STD: [0.225, 0.225, 0.225]
  TARGET_FPS: 30
  TEST_CROP_SIZE: 256
  TRAIN_CROP_SIZE: 224
  TRAIN_JITTER_SCALES: [256, 320]
DATA_LOADER:
  ENABLE_MULTI_THREAD_DECODE: False
  NUM_WORKERS: 1
  PIN_MEMORY: True
DEMO:
  BUFFER_SIZE: 0
  CLIP_VIS_SIZE: 10
  COMMON_CLASS_NAMES: ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)']
  COMMON_CLASS_THRES: 0.7
  DETECTRON2_CFG: COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml
  DETECTRON2_THRESH: 0.9
  DETECTRON2_WEIGHTS: detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl
  DISPLAY_HEIGHT: 0
  DISPLAY_WIDTH: 0
  ENABLE: False
  FPS: 30
  GT_BOXES: 
  INPUT_FORMAT: BGR
  INPUT_VIDEO: 
  LABEL_FILE_PATH: 
  NUM_CLIPS_SKIP: 0
  NUM_VIS_INSTANCES: 2
  OUTPUT_FILE: 
  OUTPUT_FPS: -1
  PREDS_BOXES: 
  SLOWMO: 1
  STARTING_SECOND: 900
  THREAD_ENABLE: False
  UNCOMMON_CLASS_THRES: 0.3
  VIS_MODE: thres
  WEBCAM: -1
DETECTION:
  ALIGNED: False
  ENABLE: True
  ROI_XFORM_RESOLUTION: 7
  SPATIAL_SCALE_FACTOR: 16
DIST_BACKEND: nccl
LOG_MODEL_INFO: False
LOG_PERIOD: 10
MODEL:
  ARCH: slowfast
  DROPCONNECT_RATE: 0.0
  DROPOUT_RATE: 0.5
  FC_INIT_STD: 0.01
  FREEZE_TO: -1
  HEAD_ACT: sigmoid
  LAST_PRE_TRAIN: 
  LOSS_FUNC: bce
  MODEL_NAME: SlowFast
  MULTI_PATHWAY_ARCH: ['slowfast']
  NUM_CLASSES: 80
  SINGLE_PATHWAY_ARCH: ['c2d', 'i3d', 'slow', 'x3d']
MULTIGRID:
  BN_BASE_SIZE: 8
  DEFAULT_B: 0
  DEFAULT_S: 0
  DEFAULT_T: 0
  EPOCH_FACTOR: 1.5
  EVAL_FREQ: 3
  LONG_CYCLE: False
  LONG_CYCLE_FACTORS: [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)]
  LONG_CYCLE_SAMPLING_RATE: 0
  SHORT_CYCLE: False
  SHORT_CYCLE_FACTORS: [0.5, 0.7071067811865476]
NONLOCAL:
  GROUP: [[1, 1], [1, 1], [1, 1], [1, 1]]
  INSTANTIATION: dot_product
  LOCATION: [[[], []], [[], []], [[6, 13, 20], []], [[], []]]
  POOL: [[[2, 2, 2], [2, 2, 2]], [[2, 2, 2], [2, 2, 2]], [[2, 2, 2], [2, 2, 2]], [[2, 2, 2], [2, 2, 2]]]
NUM_GPUS: 1
NUM_SHARDS: 1
OUTPUT_DIR: /srv/beegfs02/scratch/da_action/data/output/80_test_10_500_100
RESNET:
  DEPTH: 101
  INPLACE_RELU: True
  NUM_BLOCK_TEMP_KERNEL: [[3, 3], [4, 4], [6, 6], [3, 3]]
  NUM_GROUPS: 1
  SPATIAL_DILATIONS: [[1, 1], [1, 1], [1, 1], [2, 2]]
  SPATIAL_STRIDES: [[1, 1], [2, 2], [2, 2], [1, 1]]
  STRIDE_1X1: False
  TRANS_FUNC: bottleneck_transform
  WIDTH_PER_GROUP: 64
  ZERO_INIT_FINAL_BN: True
RNG_SEED: 0
SHARD_ID: 0
SLOWFAST:
  ALPHA: 4
  BETA_INV: 8
  FUSION_CONV_CHANNEL_RATIO: 2
  FUSION_KERNEL_SZ: 5
SOLVER:
  BASE_LR: 0.1
  BASE_LR_SCALE_NUM_SHARDS: False
  COSINE_END_LR: 0.0
  DAMPENING: 0.0
  GAMMA: 0.1
  LRS: []
  LR_POLICY: cosine
  MAX_EPOCH: 300
  MOMENTUM: 0.9
  NESTEROV: True
  OPTIMIZING_METHOD: sgd
  STEPS: []
  STEP_SIZE: 1
  WARMUP_EPOCHS: 0.0
  WARMUP_FACTOR: 0.1
  WARMUP_START_LR: 0.01
  WEIGHT_DECAY: 1e-07
TENSORBOARD:
  CATEGORIES_PATH: 
  CLASS_NAMES_PATH: 
  CONFUSION_MATRIX:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
  ENABLE: False
  HISTOGRAM:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
    TOPK: 10
  LOG_DIR: 
  MODEL_VIS:
    ACTIVATIONS: False
    COLORMAP: Pastel2
    ENABLE: False
    GRAD_CAM:
      COLORMAP: viridis
      ENABLE: True
      LAYER_LIST: []
      USE_TRUE_LABEL: False
    INPUT_VIDEO: False
    LAYER_LIST: []
    MODEL_WEIGHTS: False
    TOPK_PREDS: 1
  PREDICTIONS_PATH: 
  WRONG_PRED_VIS:
    ENABLE: False
    SUBSET_PATH: 
    TAG: Incorrectly classified videos.
TEST:
  BATCH_SIZE: 1
  CHECKPOINT_FILE_PATH: /srv/beegfs02/scratch/da_action/data/models_pretrained/SLOWFAST_32x2_R101_50_50.pkl
  CHECKPOINT_TYPE: pytorch
  DATASET: ava
  DETECTIONS_PATH: /home/sieberl/SA2020/pyslowfast/experiments/80_test_10_500_100/kinetics_detections_latest.csv
  ENABLE: True
  GT_PATH: /home/sieberl/SA2020/pyslowfast/experiments/80_test_10_500_100/kinetics_groundtruth_latest.csv
  NUM_ENSEMBLE_VIEWS: 10
  NUM_SPATIAL_CROPS: 3
  SAVE_RESULTS_PATH: 
TRAIN:
  AUTO_RESUME: True
  BATCH_SIZE: 16
  CHECKPOINT_CLEAR_NAME_PATTERN: ()
  CHECKPOINT_EPOCH_RESET: False
  CHECKPOINT_FILE_PATH: 
  CHECKPOINT_INFLATE: False
  CHECKPOINT_PERIOD: 1
  CHECKPOINT_TYPE: pytorch
  DATASET: ava
  ENABLE: False
  EVAL_PERIOD: 1
X3D:
  BN_LIN5: False
  BOTTLENECK_FACTOR: 1.0
  CHANNELWISE_3x3x3: True
  DEPTH_FACTOR: 1.0
  DIM_C1: 12
  DIM_C5: 2048
  SCALE_RES2: False
  WIDTH_FACTOR: 1.0
[12/30 21:46:46][INFO] checkpoint.py: 215: Loading network weights from /srv/beegfs02/scratch/da_action/data/models_pretrained/SLOWFAST_32x2_R101_50_50.pkl.
[12/30 21:46:52][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/kinetics700/80_frame_lists_10_500_100/val.csv
[12/30 21:46:52][INFO] ava_helper.py: 111: Finished loading annotations from: /srv/beegfs02/scratch/da_action/data/kinetics700/80_annotations_10_500_100/kinetics_val_predicted_boxes.csv
[12/30 21:46:52][INFO] ava_helper.py: 113: Detection threshold: 0.8
[12/30 21:46:52][INFO] ava_helper.py: 114: Number of unique boxes: 1476
[12/30 21:46:52][INFO] ava_helper.py: 115: Number of annotations: 0
[12/30 21:46:52][INFO] ava_helper.py: 162: 614 keyframes used.
[12/30 21:46:52][INFO] ava_dataset.py:  94: === AVA dataset summary ===
[12/30 21:46:52][INFO] ava_dataset.py:  95: Split: test
[12/30 21:46:52][INFO] ava_dataset.py:  96: Number of videos: 639
[12/30 21:46:52][INFO] ava_dataset.py: 100: Number of frames: 115020
[12/30 21:46:52][INFO] ava_dataset.py: 101: Number of key frames: 614
[12/30 21:46:52][INFO] ava_dataset.py: 102: Number of boxes: 1476.
[12/30 21:46:52][INFO] test_net.py: 189: Testing model for 614 iterations
[12/30 21:46:53][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/kinetics700/80_frame_lists_10_500_100/val.csv
[12/30 21:55:32][INFO] ava_eval_helper.py: 165: Evaluating with 639 unique GT frames.
[12/30 21:55:32][INFO] ava_eval_helper.py: 167: Evaluating with 614 unique detection frames
[12/30 21:55:33][INFO] ava_eval_helper.py: 322: AVA results wrote to /home/sieberl/SA2020/pyslowfast/experiments/80_test_10_500_100/kinetics_detections_latest.csv
[12/30 21:55:33][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[12/30 21:55:33][INFO] ava_eval_helper.py: 322: AVA results wrote to /home/sieberl/SA2020/pyslowfast/experiments/80_test_10_500_100/kinetics_groundtruth_latest.csv
[12/30 21:55:33][INFO] ava_eval_helper.py: 323: 	took 0 seconds.
[12/30 21:55:41][INFO] object_detection_evaluation.py: 772: The following classes have no ground truth examples: [ 2  3  4  5  6  7  9 10 12 13 15 16 18 19 20 21 22 23 24 25 26 27 28 29
 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 50 51 52 53 54
 55 56 57 58 60 61 62 63 64 65 66 67 68 69 70 71 72 73 75 76 77 78]
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/answer phone': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.3822237487629738,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/brush teeth': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.15193805871115085,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/catch (an object)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/chop': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/climb (e.g., a mountain)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/clink glass': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/close (e.g., a door, a box)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cook': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crawl': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crouch/kneel': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cut': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dance': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dig': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dress/put on clothing': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drink': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drive (e.g., a car, a truck)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/eat': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/enter': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/exit': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/extract': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fall down': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fight/hit (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fishing': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/get up': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/give/serve (an object) to (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/grab (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand clap': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand shake': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand wave': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hit (an object)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hug (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/jump/leap': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kick (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kick (an object)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kiss (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.5619862333520371,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift/pick up': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen (e.g., to music)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.17052372046421554,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/martial art': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/open (e.g., a window, a car door)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/paint': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play board game': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play musical instrument': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play with kids': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play with pets': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/point to (an object)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/press': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pull (an object)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (an object)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (another person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/put down': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/read': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.505219024357511,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/row boat': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/run/jog': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sail boat': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shoot': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shovel': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sing to (e.g., self, a person, a group)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.21020545158493592,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/smoke': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stand': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stir': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/swim': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take (an object) from (a person)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take a photo': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.4221408295790888,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/text on/look at a cellphone': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/throw': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.3010580726693276,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/turn (e.g., a screwdriver)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.26813362572183175,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.11817864421500707,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (e.g., TV)': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/work on a computer': nan,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/write': nan,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.30916074094180795}
[12/30 21:55:41][INFO] ava_eval_helper.py: 182: AVA eval done in 8.975565 seconds.
[12/30 21:55:41][INFO] logging.py:  97: json_stats: {
  "map": 0.30916,
  "mode": "test"
}
