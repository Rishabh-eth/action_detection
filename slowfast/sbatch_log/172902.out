
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


[11/24 23:47:24][INFO] train_net.py: 405: Train with config:
[11/24 23:47:24][INFO] train_net.py: 406: {'AVA': {'ANNOTATION_DIR': '/srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/',
         'BGR': False,
         'DETECTION_SCORE_THRESH': 0.8,
         'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv',
         'FRAME_DIR': '/srv/beegfs02/scratch/da_action/data/ava/frames/',
         'FRAME_LIST_DIR': '/srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/',
         'FULL_TEST_ON_VAL': True,
         'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv',
         'IMG_PROC_BACKEND': 'cv2',
         'LABEL_MAP_FILE': 'ava_action_list_v2.2.pbtxt',
         'TEST_FORCE_FLIP': False,
         'TEST_LISTS': ['val.csv'],
         'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'],
         'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'],
         'TRAIN_LISTS': ['train.csv'],
         'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229],
         'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009],
                              [-0.5808, -0.0045, -0.814],
                              [-0.5836, -0.6948, 0.4203]],
         'TRAIN_PCA_JITTER_ONLY': True,
         'TRAIN_PREDICT_BOX_LISTS': [],
         'TRAIN_USE_COLOR_AUGMENTATION': False},
 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}),
 'BN': {'NORM_TYPE': 'batchnorm',
        'NUM_BATCHES_PRECISE': 200,
        'NUM_SPLITS': 1,
        'NUM_SYNC_DEVICES': 1,
        'USE_PRECISE_STATS': False,
        'WEIGHT_DECAY': 0.0},
 'DATA': {'DECODING_BACKEND': 'pyav',
          'ENSEMBLE_METHOD': 'sum',
          'INPUT_CHANNEL_NUM': [3, 3],
          'INV_UNIFORM_SAMPLE': False,
          'MEAN': [0.45, 0.45, 0.45],
          'MULTI_LABEL': False,
          'NUM_FRAMES': 32,
          'PATH_LABEL_SEPARATOR': ' ',
          'PATH_PREFIX': '',
          'PATH_TO_DATA_DIR': '',
          'RANDOM_FLIP': True,
          'REVERSE_INPUT_CHANNEL': False,
          'SAMPLING_RATE': 2,
          'STD': [0.225, 0.225, 0.225],
          'TARGET_FPS': 30,
          'TEST_CROP_SIZE': 256,
          'TRAIN_CROP_SIZE': 224,
          'TRAIN_JITTER_SCALES': [256, 320]},
 'DATA_LOADER': {'ENABLE_MULTI_THREAD_DECODE': False,
                 'NUM_WORKERS': 1,
                 'PIN_MEMORY': True},
 'DEMO': {'BUFFER_SIZE': 0,
          'CLIP_VIS_SIZE': 10,
          'COMMON_CLASS_NAMES': ['watch (a person)',
                                 'talk to (e.g., self, a person, a group)',
                                 'listen to (a person)',
                                 'touch (an object)',
                                 'carry/hold (an object)',
                                 'walk',
                                 'sit',
                                 'lie/sleep',
                                 'bend/bow (at the waist)'],
          'COMMON_CLASS_THRES': 0.7,
          'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',
          'DETECTRON2_THRESH': 0.9,
          'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',
          'DISPLAY_HEIGHT': 0,
          'DISPLAY_WIDTH': 0,
          'ENABLE': False,
          'FPS': 30,
          'GT_BOXES': '',
          'INPUT_FORMAT': 'BGR',
          'INPUT_VIDEO': '',
          'LABEL_FILE_PATH': '',
          'NUM_CLIPS_SKIP': 0,
          'NUM_VIS_INSTANCES': 2,
          'OUTPUT_FILE': '',
          'OUTPUT_FPS': -1,
          'PREDS_BOXES': '',
          'SLOWMO': 1,
          'STARTING_SECOND': 900,
          'THREAD_ENABLE': False,
          'UNCOMMON_CLASS_THRES': 0.3,
          'VIS_MODE': 'thres',
          'WEBCAM': -1},
 'DETECTION': {'ALIGNED': False,
               'ENABLE': True,
               'ROI_XFORM_RESOLUTION': 7,
               'SPATIAL_SCALE_FACTOR': 16},
 'DIST_BACKEND': 'nccl',
 'LOG_MODEL_INFO': False,
 'LOG_PERIOD': 10,
 'MODEL': {'ARCH': 'slowfast',
           'DROPCONNECT_RATE': 0.0,
           'DROPOUT_RATE': 0.5,
           'FC_INIT_STD': 0.01,
           'FREEZE_TO': 152,
           'HEAD_ACT': 'sigmoid',
           'LOSS_FUNC': 'bce',
           'MODEL_NAME': 'SlowFast',
           'MULTI_PATHWAY_ARCH': ['slowfast'],
           'NUM_CLASSES': 10,
           'SINGLE_PATHWAY_ARCH': ['c2d', 'i3d', 'slow', 'x3d']},
 'MULTIGRID': {'BN_BASE_SIZE': 8,
               'DEFAULT_B': 0,
               'DEFAULT_S': 0,
               'DEFAULT_T': 0,
               'EPOCH_FACTOR': 1.5,
               'EVAL_FREQ': 3,
               'LONG_CYCLE': False,
               'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476),
                                      (0.5, 0.7071067811865476),
                                      (0.5, 1),
                                      (1, 1)],
               'LONG_CYCLE_SAMPLING_RATE': 0,
               'SHORT_CYCLE': False,
               'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476]},
 'NONLOCAL': {'GROUP': [[1, 1], [1, 1], [1, 1], [1, 1]],
              'INSTANTIATION': 'dot_product',
              'LOCATION': [[[], []], [[], []], [[6, 13, 20], []], [[], []]],
              'POOL': [[[2, 2, 2], [2, 2, 2]],
                       [[2, 2, 2], [2, 2, 2]],
                       [[2, 2, 2], [2, 2, 2]],
                       [[2, 2, 2], [2, 2, 2]]]},
 'NUM_GPUS': 1,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': '/srv/beegfs02/scratch/da_action/data/output/ex_10_500_100_v2',
 'RESNET': {'DEPTH': 101,
            'INPLACE_RELU': True,
            'NUM_BLOCK_TEMP_KERNEL': [[3, 3], [4, 4], [6, 6], [3, 3]],
            'NUM_GROUPS': 1,
            'SPATIAL_DILATIONS': [[1, 1], [1, 1], [1, 1], [2, 2]],
            'SPATIAL_STRIDES': [[1, 1], [2, 2], [2, 2], [1, 1]],
            'STRIDE_1X1': False,
            'TRANS_FUNC': 'bottleneck_transform',
            'WIDTH_PER_GROUP': 64,
            'ZERO_INIT_FINAL_BN': True},
 'RNG_SEED': 0,
 'SHARD_ID': 0,
 'SLOWFAST': {'ALPHA': 4,
              'BETA_INV': 8,
              'FUSION_CONV_CHANNEL_RATIO': 2,
              'FUSION_KERNEL_SZ': 5},
 'SOLVER': {'BASE_LR': 0.1,
            'BASE_LR_SCALE_NUM_SHARDS': False,
            'COSINE_END_LR': 0.0,
            'DAMPENING': 0.0,
            'GAMMA': 0.1,
            'LRS': [],
            'LR_POLICY': 'cosine',
            'MAX_EPOCH': 300,
            'MOMENTUM': 0.9,
            'NESTEROV': True,
            'OPTIMIZING_METHOD': 'sgd',
            'STEPS': [],
            'STEP_SIZE': 1,
            'WARMUP_EPOCHS': 0.0,
            'WARMUP_FACTOR': 0.1,
            'WARMUP_START_LR': 0.01,
            'WEIGHT_DECAY': 1e-07},
 'TENSORBOARD': {'CATEGORIES_PATH': '',
                 'CLASS_NAMES_PATH': '',
                 'CONFUSION_MATRIX': {'ENABLE': False,
                                      'FIGSIZE': [8, 8],
                                      'SUBSET_PATH': ''},
                 'ENABLE': True,
                 'HISTOGRAM': {'ENABLE': False,
                               'FIGSIZE': [8, 8],
                               'SUBSET_PATH': '',
                               'TOPK': 10},
                 'LOG_DIR': 'tensorboard',
                 'MODEL_VIS': {'ACTIVATIONS': False,
                               'COLORMAP': 'Pastel2',
                               'ENABLE': False,
                               'GRAD_CAM': {'COLORMAP': 'viridis',
                                            'ENABLE': True,
                                            'LAYER_LIST': [],
                                            'USE_TRUE_LABEL': False},
                               'INPUT_VIDEO': False,
                               'LAYER_LIST': [],
                               'MODEL_WEIGHTS': False,
                               'TOPK_PREDS': 1},
                 'PREDICTIONS_PATH': '',
                 'WRONG_PRED_VIS': {'ENABLE': False,
                                    'SUBSET_PATH': '',
                                    'TAG': 'Incorrectly classified videos.'}},
 'TEST': {'BATCH_SIZE': 4,
          'CHECKPOINT_FILE_PATH': '',
          'CHECKPOINT_TYPE': 'pytorch',
          'DATASET': 'ava',
          'ENABLE': True,
          'NUM_ENSEMBLE_VIEWS': 10,
          'NUM_SPATIAL_CROPS': 3,
          'SAVE_RESULTS_PATH': ''},
 'TRAIN': {'AUTO_RESUME': False,
           'BATCH_SIZE': 4,
           'CHECKPOINT_CLEAR_NAME_PATTERN': (),
           'CHECKPOINT_EPOCH_RESET': False,
           'CHECKPOINT_FILE_PATH': '/srv/beegfs02/scratch/da_action/data/models_pretrained/SLOWFAST_32x2_R101_50_50.pkl',
           'CHECKPOINT_INFLATE': False,
           'CHECKPOINT_PERIOD': 1,
           'CHECKPOINT_TYPE': 'pytorch',
           'DATASET': 'ava',
           'ENABLE': True,
           'EVAL_PERIOD': 1},
 'X3D': {'BN_LIN5': False,
         'BOTTLENECK_FACTOR': 1.0,
         'CHANNELWISE_3x3x3': True,
         'DEPTH_FACTOR': 1.0,
         'DIM_C1': 12,
         'DIM_C5': 2048,
         'SCALE_RES2': False,
         'WIDTH_FACTOR': 1.0}}
0 frozen s1.pathway0_stem.conv.weight
1 frozen s1.pathway0_stem.bn.weight
2 frozen s1.pathway0_stem.bn.bias
3 frozen s1.pathway1_stem.conv.weight
4 frozen s1.pathway1_stem.bn.weight
5 frozen s1.pathway1_stem.bn.bias
6 frozen s1_fuse.conv_f2s.weight
7 frozen s1_fuse.bn.weight
8 frozen s1_fuse.bn.bias
9 frozen s2.pathway0_res0.branch1.weight
10 frozen s2.pathway0_res0.branch1_bn.weight
11 frozen s2.pathway0_res0.branch1_bn.bias
12 frozen s2.pathway0_res0.branch2.a.weight
13 frozen s2.pathway0_res0.branch2.a_bn.weight
14 frozen s2.pathway0_res0.branch2.a_bn.bias
15 frozen s2.pathway0_res0.branch2.b.weight
16 frozen s2.pathway0_res0.branch2.b_bn.weight
17 frozen s2.pathway0_res0.branch2.b_bn.bias
18 frozen s2.pathway0_res0.branch2.c.weight
19 frozen s2.pathway0_res0.branch2.c_bn.weight
20 frozen s2.pathway0_res0.branch2.c_bn.bias
21 frozen s2.pathway0_res1.branch2.a.weight
22 frozen s2.pathway0_res1.branch2.a_bn.weight
23 frozen s2.pathway0_res1.branch2.a_bn.bias
24 frozen s2.pathway0_res1.branch2.b.weight
25 frozen s2.pathway0_res1.branch2.b_bn.weight
26 frozen s2.pathway0_res1.branch2.b_bn.bias
27 frozen s2.pathway0_res1.branch2.c.weight
28 frozen s2.pathway0_res1.branch2.c_bn.weight
29 frozen s2.pathway0_res1.branch2.c_bn.bias
30 frozen s2.pathway0_res2.branch2.a.weight
31 frozen s2.pathway0_res2.branch2.a_bn.weight
32 frozen s2.pathway0_res2.branch2.a_bn.bias
33 frozen s2.pathway0_res2.branch2.b.weight
34 frozen s2.pathway0_res2.branch2.b_bn.weight
35 frozen s2.pathway0_res2.branch2.b_bn.bias
36 frozen s2.pathway0_res2.branch2.c.weight
37 frozen s2.pathway0_res2.branch2.c_bn.weight
38 frozen s2.pathway0_res2.branch2.c_bn.bias
39 frozen s2.pathway1_res0.branch1.weight
40 frozen s2.pathway1_res0.branch1_bn.weight
41 frozen s2.pathway1_res0.branch1_bn.bias
42 frozen s2.pathway1_res0.branch2.a.weight
43 frozen s2.pathway1_res0.branch2.a_bn.weight
44 frozen s2.pathway1_res0.branch2.a_bn.bias
45 frozen s2.pathway1_res0.branch2.b.weight
46 frozen s2.pathway1_res0.branch2.b_bn.weight
47 frozen s2.pathway1_res0.branch2.b_bn.bias
48 frozen s2.pathway1_res0.branch2.c.weight
49 frozen s2.pathway1_res0.branch2.c_bn.weight
50 frozen s2.pathway1_res0.branch2.c_bn.bias
51 frozen s2.pathway1_res1.branch2.a.weight
52 frozen s2.pathway1_res1.branch2.a_bn.weight
53 frozen s2.pathway1_res1.branch2.a_bn.bias
54 frozen s2.pathway1_res1.branch2.b.weight
55 frozen s2.pathway1_res1.branch2.b_bn.weight
56 frozen s2.pathway1_res1.branch2.b_bn.bias
57 frozen s2.pathway1_res1.branch2.c.weight
58 frozen s2.pathway1_res1.branch2.c_bn.weight
59 frozen s2.pathway1_res1.branch2.c_bn.bias
60 frozen s2.pathway1_res2.branch2.a.weight
61 frozen s2.pathway1_res2.branch2.a_bn.weight
62 frozen s2.pathway1_res2.branch2.a_bn.bias
63 frozen s2.pathway1_res2.branch2.b.weight
64 frozen s2.pathway1_res2.branch2.b_bn.weight
65 frozen s2.pathway1_res2.branch2.b_bn.bias
66 frozen s2.pathway1_res2.branch2.c.weight
67 frozen s2.pathway1_res2.branch2.c_bn.weight
68 frozen s2.pathway1_res2.branch2.c_bn.bias
69 frozen s2_fuse.conv_f2s.weight
70 frozen s2_fuse.bn.weight
71 frozen s2_fuse.bn.bias
72 frozen s3.pathway0_res0.branch1.weight
73 frozen s3.pathway0_res0.branch1_bn.weight
74 frozen s3.pathway0_res0.branch1_bn.bias
75 frozen s3.pathway0_res0.branch2.a.weight
76 frozen s3.pathway0_res0.branch2.a_bn.weight
77 frozen s3.pathway0_res0.branch2.a_bn.bias
78 frozen s3.pathway0_res0.branch2.b.weight
79 frozen s3.pathway0_res0.branch2.b_bn.weight
80 frozen s3.pathway0_res0.branch2.b_bn.bias
81 frozen s3.pathway0_res0.branch2.c.weight
82 frozen s3.pathway0_res0.branch2.c_bn.weight
83 frozen s3.pathway0_res0.branch2.c_bn.bias
84 frozen s3.pathway0_res1.branch2.a.weight
85 frozen s3.pathway0_res1.branch2.a_bn.weight
86 frozen s3.pathway0_res1.branch2.a_bn.bias
87 frozen s3.pathway0_res1.branch2.b.weight
88 frozen s3.pathway0_res1.branch2.b_bn.weight
89 frozen s3.pathway0_res1.branch2.b_bn.bias
90 frozen s3.pathway0_res1.branch2.c.weight
91 frozen s3.pathway0_res1.branch2.c_bn.weight
92 frozen s3.pathway0_res1.branch2.c_bn.bias
93 frozen s3.pathway0_res2.branch2.a.weight
94 frozen s3.pathway0_res2.branch2.a_bn.weight
95 frozen s3.pathway0_res2.branch2.a_bn.bias
96 frozen s3.pathway0_res2.branch2.b.weight
97 frozen s3.pathway0_res2.branch2.b_bn.weight
98 frozen s3.pathway0_res2.branch2.b_bn.bias
99 frozen s3.pathway0_res2.branch2.c.weight
100 frozen s3.pathway0_res2.branch2.c_bn.weight
101 frozen s3.pathway0_res2.branch2.c_bn.bias
102 frozen s3.pathway0_res3.branch2.a.weight
103 frozen s3.pathway0_res3.branch2.a_bn.weight
104 frozen s3.pathway0_res3.branch2.a_bn.bias
105 frozen s3.pathway0_res3.branch2.b.weight
106 frozen s3.pathway0_res3.branch2.b_bn.weight
107 frozen s3.pathway0_res3.branch2.b_bn.bias
108 frozen s3.pathway0_res3.branch2.c.weight
109 frozen s3.pathway0_res3.branch2.c_bn.weight
110 frozen s3.pathway0_res3.branch2.c_bn.bias
111 frozen s3.pathway1_res0.branch1.weight
112 frozen s3.pathway1_res0.branch1_bn.weight
113 frozen s3.pathway1_res0.branch1_bn.bias
114 frozen s3.pathway1_res0.branch2.a.weight
115 frozen s3.pathway1_res0.branch2.a_bn.weight
116 frozen s3.pathway1_res0.branch2.a_bn.bias
117 frozen s3.pathway1_res0.branch2.b.weight
118 frozen s3.pathway1_res0.branch2.b_bn.weight
119 frozen s3.pathway1_res0.branch2.b_bn.bias
120 frozen s3.pathway1_res0.branch2.c.weight
121 frozen s3.pathway1_res0.branch2.c_bn.weight
122 frozen s3.pathway1_res0.branch2.c_bn.bias
123 frozen s3.pathway1_res1.branch2.a.weight
124 frozen s3.pathway1_res1.branch2.a_bn.weight
125 frozen s3.pathway1_res1.branch2.a_bn.bias
126 frozen s3.pathway1_res1.branch2.b.weight
127 frozen s3.pathway1_res1.branch2.b_bn.weight
128 frozen s3.pathway1_res1.branch2.b_bn.bias
129 frozen s3.pathway1_res1.branch2.c.weight
130 frozen s3.pathway1_res1.branch2.c_bn.weight
131 frozen s3.pathway1_res1.branch2.c_bn.bias
132 frozen s3.pathway1_res2.branch2.a.weight
133 frozen s3.pathway1_res2.branch2.a_bn.weight
134 frozen s3.pathway1_res2.branch2.a_bn.bias
135 frozen s3.pathway1_res2.branch2.b.weight
136 frozen s3.pathway1_res2.branch2.b_bn.weight
137 frozen s3.pathway1_res2.branch2.b_bn.bias
138 frozen s3.pathway1_res2.branch2.c.weight
139 frozen s3.pathway1_res2.branch2.c_bn.weight
140 frozen s3.pathway1_res2.branch2.c_bn.bias
141 frozen s3.pathway1_res3.branch2.a.weight
142 frozen s3.pathway1_res3.branch2.a_bn.weight
143 frozen s3.pathway1_res3.branch2.a_bn.bias
144 frozen s3.pathway1_res3.branch2.b.weight
145 frozen s3.pathway1_res3.branch2.b_bn.weight
146 frozen s3.pathway1_res3.branch2.b_bn.bias
147 frozen s3.pathway1_res3.branch2.c.weight
148 frozen s3.pathway1_res3.branch2.c_bn.weight
149 frozen s3.pathway1_res3.branch2.c_bn.bias
150 frozen s3_fuse.conv_f2s.weight
151 frozen s3_fuse.bn.weight
152 frozen s3_fuse.bn.bias
153 unfrozen s4.pathway0_res0.branch1.weight
154 unfrozen s4.pathway0_res0.branch1_bn.weight
155 unfrozen s4.pathway0_res0.branch1_bn.bias
156 unfrozen s4.pathway0_res0.branch2.a.weight
157 unfrozen s4.pathway0_res0.branch2.a_bn.weight
158 unfrozen s4.pathway0_res0.branch2.a_bn.bias
159 unfrozen s4.pathway0_res0.branch2.b.weight
160 unfrozen s4.pathway0_res0.branch2.b_bn.weight
161 unfrozen s4.pathway0_res0.branch2.b_bn.bias
162 unfrozen s4.pathway0_res0.branch2.c.weight
163 unfrozen s4.pathway0_res0.branch2.c_bn.weight
164 unfrozen s4.pathway0_res0.branch2.c_bn.bias
165 unfrozen s4.pathway0_res1.branch2.a.weight
166 unfrozen s4.pathway0_res1.branch2.a_bn.weight
167 unfrozen s4.pathway0_res1.branch2.a_bn.bias
168 unfrozen s4.pathway0_res1.branch2.b.weight
169 unfrozen s4.pathway0_res1.branch2.b_bn.weight
170 unfrozen s4.pathway0_res1.branch2.b_bn.bias
171 unfrozen s4.pathway0_res1.branch2.c.weight
172 unfrozen s4.pathway0_res1.branch2.c_bn.weight
173 unfrozen s4.pathway0_res1.branch2.c_bn.bias
174 unfrozen s4.pathway0_res2.branch2.a.weight
175 unfrozen s4.pathway0_res2.branch2.a_bn.weight
176 unfrozen s4.pathway0_res2.branch2.a_bn.bias
177 unfrozen s4.pathway0_res2.branch2.b.weight
178 unfrozen s4.pathway0_res2.branch2.b_bn.weight
179 unfrozen s4.pathway0_res2.branch2.b_bn.bias
180 unfrozen s4.pathway0_res2.branch2.c.weight
181 unfrozen s4.pathway0_res2.branch2.c_bn.weight
182 unfrozen s4.pathway0_res2.branch2.c_bn.bias
183 unfrozen s4.pathway0_res3.branch2.a.weight
184 unfrozen s4.pathway0_res3.branch2.a_bn.weight
185 unfrozen s4.pathway0_res3.branch2.a_bn.bias
186 unfrozen s4.pathway0_res3.branch2.b.weight
187 unfrozen s4.pathway0_res3.branch2.b_bn.weight
188 unfrozen s4.pathway0_res3.branch2.b_bn.bias
189 unfrozen s4.pathway0_res3.branch2.c.weight
190 unfrozen s4.pathway0_res3.branch2.c_bn.weight
191 unfrozen s4.pathway0_res3.branch2.c_bn.bias
192 unfrozen s4.pathway0_res4.branch2.a.weight
193 unfrozen s4.pathway0_res4.branch2.a_bn.weight
194 unfrozen s4.pathway0_res4.branch2.a_bn.bias
195 unfrozen s4.pathway0_res4.branch2.b.weight
196 unfrozen s4.pathway0_res4.branch2.b_bn.weight
197 unfrozen s4.pathway0_res4.branch2.b_bn.bias
198 unfrozen s4.pathway0_res4.branch2.c.weight
199 unfrozen s4.pathway0_res4.branch2.c_bn.weight
200 unfrozen s4.pathway0_res4.branch2.c_bn.bias
201 unfrozen s4.pathway0_res5.branch2.a.weight
202 unfrozen s4.pathway0_res5.branch2.a_bn.weight
203 unfrozen s4.pathway0_res5.branch2.a_bn.bias
204 unfrozen s4.pathway0_res5.branch2.b.weight
205 unfrozen s4.pathway0_res5.branch2.b_bn.weight
206 unfrozen s4.pathway0_res5.branch2.b_bn.bias
207 unfrozen s4.pathway0_res5.branch2.c.weight
208 unfrozen s4.pathway0_res5.branch2.c_bn.weight
209 unfrozen s4.pathway0_res5.branch2.c_bn.bias
210 unfrozen s4.pathway0_res6.branch2.a.weight
211 unfrozen s4.pathway0_res6.branch2.a_bn.weight
212 unfrozen s4.pathway0_res6.branch2.a_bn.bias
213 unfrozen s4.pathway0_res6.branch2.b.weight
214 unfrozen s4.pathway0_res6.branch2.b_bn.weight
215 unfrozen s4.pathway0_res6.branch2.b_bn.bias
216 unfrozen s4.pathway0_res6.branch2.c.weight
217 unfrozen s4.pathway0_res6.branch2.c_bn.weight
218 unfrozen s4.pathway0_res6.branch2.c_bn.bias
219 unfrozen s4.pathway0_nonlocal6.conv_theta.weight
220 unfrozen s4.pathway0_nonlocal6.conv_theta.bias
221 unfrozen s4.pathway0_nonlocal6.conv_phi.weight
222 unfrozen s4.pathway0_nonlocal6.conv_phi.bias
223 unfrozen s4.pathway0_nonlocal6.conv_g.weight
224 unfrozen s4.pathway0_nonlocal6.conv_g.bias
225 unfrozen s4.pathway0_nonlocal6.conv_out.weight
226 unfrozen s4.pathway0_nonlocal6.conv_out.bias
227 unfrozen s4.pathway0_nonlocal6.bn.weight
228 unfrozen s4.pathway0_nonlocal6.bn.bias
229 unfrozen s4.pathway0_res7.branch2.a.weight
230 unfrozen s4.pathway0_res7.branch2.a_bn.weight
231 unfrozen s4.pathway0_res7.branch2.a_bn.bias
232 unfrozen s4.pathway0_res7.branch2.b.weight
233 unfrozen s4.pathway0_res7.branch2.b_bn.weight
234 unfrozen s4.pathway0_res7.branch2.b_bn.bias
235 unfrozen s4.pathway0_res7.branch2.c.weight
236 unfrozen s4.pathway0_res7.branch2.c_bn.weight
237 unfrozen s4.pathway0_res7.branch2.c_bn.bias
238 unfrozen s4.pathway0_res8.branch2.a.weight
239 unfrozen s4.pathway0_res8.branch2.a_bn.weight
240 unfrozen s4.pathway0_res8.branch2.a_bn.bias
241 unfrozen s4.pathway0_res8.branch2.b.weight
242 unfrozen s4.pathway0_res8.branch2.b_bn.weight
243 unfrozen s4.pathway0_res8.branch2.b_bn.bias
244 unfrozen s4.pathway0_res8.branch2.c.weight
245 unfrozen s4.pathway0_res8.branch2.c_bn.weight
246 unfrozen s4.pathway0_res8.branch2.c_bn.bias
247 unfrozen s4.pathway0_res9.branch2.a.weight
248 unfrozen s4.pathway0_res9.branch2.a_bn.weight
249 unfrozen s4.pathway0_res9.branch2.a_bn.bias
250 unfrozen s4.pathway0_res9.branch2.b.weight
251 unfrozen s4.pathway0_res9.branch2.b_bn.weight
252 unfrozen s4.pathway0_res9.branch2.b_bn.bias
253 unfrozen s4.pathway0_res9.branch2.c.weight
254 unfrozen s4.pathway0_res9.branch2.c_bn.weight
255 unfrozen s4.pathway0_res9.branch2.c_bn.bias
256 unfrozen s4.pathway0_res10.branch2.a.weight
257 unfrozen s4.pathway0_res10.branch2.a_bn.weight
258 unfrozen s4.pathway0_res10.branch2.a_bn.bias
259 unfrozen s4.pathway0_res10.branch2.b.weight
260 unfrozen s4.pathway0_res10.branch2.b_bn.weight
261 unfrozen s4.pathway0_res10.branch2.b_bn.bias
262 unfrozen s4.pathway0_res10.branch2.c.weight
263 unfrozen s4.pathway0_res10.branch2.c_bn.weight
264 unfrozen s4.pathway0_res10.branch2.c_bn.bias
265 unfrozen s4.pathway0_res11.branch2.a.weight
266 unfrozen s4.pathway0_res11.branch2.a_bn.weight
267 unfrozen s4.pathway0_res11.branch2.a_bn.bias
268 unfrozen s4.pathway0_res11.branch2.b.weight
269 unfrozen s4.pathway0_res11.branch2.b_bn.weight
270 unfrozen s4.pathway0_res11.branch2.b_bn.bias
271 unfrozen s4.pathway0_res11.branch2.c.weight
272 unfrozen s4.pathway0_res11.branch2.c_bn.weight
273 unfrozen s4.pathway0_res11.branch2.c_bn.bias
274 unfrozen s4.pathway0_res12.branch2.a.weight
275 unfrozen s4.pathway0_res12.branch2.a_bn.weight
276 unfrozen s4.pathway0_res12.branch2.a_bn.bias
277 unfrozen s4.pathway0_res12.branch2.b.weight
278 unfrozen s4.pathway0_res12.branch2.b_bn.weight
279 unfrozen s4.pathway0_res12.branch2.b_bn.bias
280 unfrozen s4.pathway0_res12.branch2.c.weight
281 unfrozen s4.pathway0_res12.branch2.c_bn.weight
282 unfrozen s4.pathway0_res12.branch2.c_bn.bias
283 unfrozen s4.pathway0_res13.branch2.a.weight
284 unfrozen s4.pathway0_res13.branch2.a_bn.weight
285 unfrozen s4.pathway0_res13.branch2.a_bn.bias
286 unfrozen s4.pathway0_res13.branch2.b.weight
287 unfrozen s4.pathway0_res13.branch2.b_bn.weight
288 unfrozen s4.pathway0_res13.branch2.b_bn.bias
289 unfrozen s4.pathway0_res13.branch2.c.weight
290 unfrozen s4.pathway0_res13.branch2.c_bn.weight
291 unfrozen s4.pathway0_res13.branch2.c_bn.bias
292 unfrozen s4.pathway0_nonlocal13.conv_theta.weight
293 unfrozen s4.pathway0_nonlocal13.conv_theta.bias
294 unfrozen s4.pathway0_nonlocal13.conv_phi.weight
295 unfrozen s4.pathway0_nonlocal13.conv_phi.bias
296 unfrozen s4.pathway0_nonlocal13.conv_g.weight
297 unfrozen s4.pathway0_nonlocal13.conv_g.bias
298 unfrozen s4.pathway0_nonlocal13.conv_out.weight
299 unfrozen s4.pathway0_nonlocal13.conv_out.bias
300 unfrozen s4.pathway0_nonlocal13.bn.weight
301 unfrozen s4.pathway0_nonlocal13.bn.bias
302 unfrozen s4.pathway0_res14.branch2.a.weight
303 unfrozen s4.pathway0_res14.branch2.a_bn.weight
304 unfrozen s4.pathway0_res14.branch2.a_bn.bias
305 unfrozen s4.pathway0_res14.branch2.b.weight
306 unfrozen s4.pathway0_res14.branch2.b_bn.weight
307 unfrozen s4.pathway0_res14.branch2.b_bn.bias
308 unfrozen s4.pathway0_res14.branch2.c.weight
309 unfrozen s4.pathway0_res14.branch2.c_bn.weight
310 unfrozen s4.pathway0_res14.branch2.c_bn.bias
311 unfrozen s4.pathway0_res15.branch2.a.weight
312 unfrozen s4.pathway0_res15.branch2.a_bn.weight
313 unfrozen s4.pathway0_res15.branch2.a_bn.bias
314 unfrozen s4.pathway0_res15.branch2.b.weight
315 unfrozen s4.pathway0_res15.branch2.b_bn.weight
316 unfrozen s4.pathway0_res15.branch2.b_bn.bias
317 unfrozen s4.pathway0_res15.branch2.c.weight
318 unfrozen s4.pathway0_res15.branch2.c_bn.weight
319 unfrozen s4.pathway0_res15.branch2.c_bn.bias
320 unfrozen s4.pathway0_res16.branch2.a.weight
321 unfrozen s4.pathway0_res16.branch2.a_bn.weight
322 unfrozen s4.pathway0_res16.branch2.a_bn.bias
323 unfrozen s4.pathway0_res16.branch2.b.weight
324 unfrozen s4.pathway0_res16.branch2.b_bn.weight
325 unfrozen s4.pathway0_res16.branch2.b_bn.bias
326 unfrozen s4.pathway0_res16.branch2.c.weight
327 unfrozen s4.pathway0_res16.branch2.c_bn.weight
328 unfrozen s4.pathway0_res16.branch2.c_bn.bias
329 unfrozen s4.pathway0_res17.branch2.a.weight
330 unfrozen s4.pathway0_res17.branch2.a_bn.weight
331 unfrozen s4.pathway0_res17.branch2.a_bn.bias
332 unfrozen s4.pathway0_res17.branch2.b.weight
333 unfrozen s4.pathway0_res17.branch2.b_bn.weight
334 unfrozen s4.pathway0_res17.branch2.b_bn.bias
335 unfrozen s4.pathway0_res17.branch2.c.weight
336 unfrozen s4.pathway0_res17.branch2.c_bn.weight
337 unfrozen s4.pathway0_res17.branch2.c_bn.bias
338 unfrozen s4.pathway0_res18.branch2.a.weight
339 unfrozen s4.pathway0_res18.branch2.a_bn.weight
340 unfrozen s4.pathway0_res18.branch2.a_bn.bias
341 unfrozen s4.pathway0_res18.branch2.b.weight
342 unfrozen s4.pathway0_res18.branch2.b_bn.weight
343 unfrozen s4.pathway0_res18.branch2.b_bn.bias
344 unfrozen s4.pathway0_res18.branch2.c.weight
345 unfrozen s4.pathway0_res18.branch2.c_bn.weight
346 unfrozen s4.pathway0_res18.branch2.c_bn.bias
347 unfrozen s4.pathway0_res19.branch2.a.weight
348 unfrozen s4.pathway0_res19.branch2.a_bn.weight
349 unfrozen s4.pathway0_res19.branch2.a_bn.bias
350 unfrozen s4.pathway0_res19.branch2.b.weight
351 unfrozen s4.pathway0_res19.branch2.b_bn.weight
352 unfrozen s4.pathway0_res19.branch2.b_bn.bias
353 unfrozen s4.pathway0_res19.branch2.c.weight
354 unfrozen s4.pathway0_res19.branch2.c_bn.weight
355 unfrozen s4.pathway0_res19.branch2.c_bn.bias
356 unfrozen s4.pathway0_res20.branch2.a.weight
357 unfrozen s4.pathway0_res20.branch2.a_bn.weight
358 unfrozen s4.pathway0_res20.branch2.a_bn.bias
359 unfrozen s4.pathway0_res20.branch2.b.weight
360 unfrozen s4.pathway0_res20.branch2.b_bn.weight
361 unfrozen s4.pathway0_res20.branch2.b_bn.bias
362 unfrozen s4.pathway0_res20.branch2.c.weight
363 unfrozen s4.pathway0_res20.branch2.c_bn.weight
364 unfrozen s4.pathway0_res20.branch2.c_bn.bias
365 unfrozen s4.pathway0_nonlocal20.conv_theta.weight
366 unfrozen s4.pathway0_nonlocal20.conv_theta.bias
367 unfrozen s4.pathway0_nonlocal20.conv_phi.weight
368 unfrozen s4.pathway0_nonlocal20.conv_phi.bias
369 unfrozen s4.pathway0_nonlocal20.conv_g.weight
370 unfrozen s4.pathway0_nonlocal20.conv_g.bias
371 unfrozen s4.pathway0_nonlocal20.conv_out.weight
372 unfrozen s4.pathway0_nonlocal20.conv_out.bias
373 unfrozen s4.pathway0_nonlocal20.bn.weight
374 unfrozen s4.pathway0_nonlocal20.bn.bias
375 unfrozen s4.pathway0_res21.branch2.a.weight
376 unfrozen s4.pathway0_res21.branch2.a_bn.weight
377 unfrozen s4.pathway0_res21.branch2.a_bn.bias
378 unfrozen s4.pathway0_res21.branch2.b.weight
379 unfrozen s4.pathway0_res21.branch2.b_bn.weight
380 unfrozen s4.pathway0_res21.branch2.b_bn.bias
381 unfrozen s4.pathway0_res21.branch2.c.weight
382 unfrozen s4.pathway0_res21.branch2.c_bn.weight
383 unfrozen s4.pathway0_res21.branch2.c_bn.bias
384 unfrozen s4.pathway0_res22.branch2.a.weight
385 unfrozen s4.pathway0_res22.branch2.a_bn.weight
386 unfrozen s4.pathway0_res22.branch2.a_bn.bias
387 unfrozen s4.pathway0_res22.branch2.b.weight
388 unfrozen s4.pathway0_res22.branch2.b_bn.weight
389 unfrozen s4.pathway0_res22.branch2.b_bn.bias
390 unfrozen s4.pathway0_res22.branch2.c.weight
391 unfrozen s4.pathway0_res22.branch2.c_bn.weight
392 unfrozen s4.pathway0_res22.branch2.c_bn.bias
393 unfrozen s4.pathway1_res0.branch1.weight
394 unfrozen s4.pathway1_res0.branch1_bn.weight
395 unfrozen s4.pathway1_res0.branch1_bn.bias
396 unfrozen s4.pathway1_res0.branch2.a.weight
397 unfrozen s4.pathway1_res0.branch2.a_bn.weight
398 unfrozen s4.pathway1_res0.branch2.a_bn.bias
399 unfrozen s4.pathway1_res0.branch2.b.weight
400 unfrozen s4.pathway1_res0.branch2.b_bn.weight
401 unfrozen s4.pathway1_res0.branch2.b_bn.bias
402 unfrozen s4.pathway1_res0.branch2.c.weight
403 unfrozen s4.pathway1_res0.branch2.c_bn.weight
404 unfrozen s4.pathway1_res0.branch2.c_bn.bias
405 unfrozen s4.pathway1_res1.branch2.a.weight
406 unfrozen s4.pathway1_res1.branch2.a_bn.weight
407 unfrozen s4.pathway1_res1.branch2.a_bn.bias
408 unfrozen s4.pathway1_res1.branch2.b.weight
409 unfrozen s4.pathway1_res1.branch2.b_bn.weight
410 unfrozen s4.pathway1_res1.branch2.b_bn.bias
411 unfrozen s4.pathway1_res1.branch2.c.weight
412 unfrozen s4.pathway1_res1.branch2.c_bn.weight
413 unfrozen s4.pathway1_res1.branch2.c_bn.bias
414 unfrozen s4.pathway1_res2.branch2.a.weight
415 unfrozen s4.pathway1_res2.branch2.a_bn.weight
416 unfrozen s4.pathway1_res2.branch2.a_bn.bias
417 unfrozen s4.pathway1_res2.branch2.b.weight
418 unfrozen s4.pathway1_res2.branch2.b_bn.weight
419 unfrozen s4.pathway1_res2.branch2.b_bn.bias
420 unfrozen s4.pathway1_res2.branch2.c.weight
421 unfrozen s4.pathway1_res2.branch2.c_bn.weight
422 unfrozen s4.pathway1_res2.branch2.c_bn.bias
423 unfrozen s4.pathway1_res3.branch2.a.weight
424 unfrozen s4.pathway1_res3.branch2.a_bn.weight
425 unfrozen s4.pathway1_res3.branch2.a_bn.bias
426 unfrozen s4.pathway1_res3.branch2.b.weight
427 unfrozen s4.pathway1_res3.branch2.b_bn.weight
428 unfrozen s4.pathway1_res3.branch2.b_bn.bias
429 unfrozen s4.pathway1_res3.branch2.c.weight
430 unfrozen s4.pathway1_res3.branch2.c_bn.weight
431 unfrozen s4.pathway1_res3.branch2.c_bn.bias
432 unfrozen s4.pathway1_res4.branch2.a.weight
433 unfrozen s4.pathway1_res4.branch2.a_bn.weight
434 unfrozen s4.pathway1_res4.branch2.a_bn.bias
435 unfrozen s4.pathway1_res4.branch2.b.weight
436 unfrozen s4.pathway1_res4.branch2.b_bn.weight
437 unfrozen s4.pathway1_res4.branch2.b_bn.bias
438 unfrozen s4.pathway1_res4.branch2.c.weight
439 unfrozen s4.pathway1_res4.branch2.c_bn.weight
440 unfrozen s4.pathway1_res4.branch2.c_bn.bias
441 unfrozen s4.pathway1_res5.branch2.a.weight
442 unfrozen s4.pathway1_res5.branch2.a_bn.weight
443 unfrozen s4.pathway1_res5.branch2.a_bn.bias
444 unfrozen s4.pathway1_res5.branch2.b.weight
445 unfrozen s4.pathway1_res5.branch2.b_bn.weight
446 unfrozen s4.pathway1_res5.branch2.b_bn.bias
447 unfrozen s4.pathway1_res5.branch2.c.weight
448 unfrozen s4.pathway1_res5.branch2.c_bn.weight
449 unfrozen s4.pathway1_res5.branch2.c_bn.bias
450 unfrozen s4.pathway1_res6.branch2.a.weight
451 unfrozen s4.pathway1_res6.branch2.a_bn.weight
452 unfrozen s4.pathway1_res6.branch2.a_bn.bias
453 unfrozen s4.pathway1_res6.branch2.b.weight
454 unfrozen s4.pathway1_res6.branch2.b_bn.weight
455 unfrozen s4.pathway1_res6.branch2.b_bn.bias
456 unfrozen s4.pathway1_res6.branch2.c.weight
457 unfrozen s4.pathway1_res6.branch2.c_bn.weight
458 unfrozen s4.pathway1_res6.branch2.c_bn.bias
459 unfrozen s4.pathway1_res7.branch2.a.weight
460 unfrozen s4.pathway1_res7.branch2.a_bn.weight
461 unfrozen s4.pathway1_res7.branch2.a_bn.bias
462 unfrozen s4.pathway1_res7.branch2.b.weight
463 unfrozen s4.pathway1_res7.branch2.b_bn.weight
464 unfrozen s4.pathway1_res7.branch2.b_bn.bias
465 unfrozen s4.pathway1_res7.branch2.c.weight
466 unfrozen s4.pathway1_res7.branch2.c_bn.weight
467 unfrozen s4.pathway1_res7.branch2.c_bn.bias
468 unfrozen s4.pathway1_res8.branch2.a.weight
469 unfrozen s4.pathway1_res8.branch2.a_bn.weight
470 unfrozen s4.pathway1_res8.branch2.a_bn.bias
471 unfrozen s4.pathway1_res8.branch2.b.weight
472 unfrozen s4.pathway1_res8.branch2.b_bn.weight
473 unfrozen s4.pathway1_res8.branch2.b_bn.bias
474 unfrozen s4.pathway1_res8.branch2.c.weight
475 unfrozen s4.pathway1_res8.branch2.c_bn.weight
476 unfrozen s4.pathway1_res8.branch2.c_bn.bias
477 unfrozen s4.pathway1_res9.branch2.a.weight
478 unfrozen s4.pathway1_res9.branch2.a_bn.weight
479 unfrozen s4.pathway1_res9.branch2.a_bn.bias
480 unfrozen s4.pathway1_res9.branch2.b.weight
481 unfrozen s4.pathway1_res9.branch2.b_bn.weight
482 unfrozen s4.pathway1_res9.branch2.b_bn.bias
483 unfrozen s4.pathway1_res9.branch2.c.weight
484 unfrozen s4.pathway1_res9.branch2.c_bn.weight
485 unfrozen s4.pathway1_res9.branch2.c_bn.bias
486 unfrozen s4.pathway1_res10.branch2.a.weight
487 unfrozen s4.pathway1_res10.branch2.a_bn.weight
488 unfrozen s4.pathway1_res10.branch2.a_bn.bias
489 unfrozen s4.pathway1_res10.branch2.b.weight
490 unfrozen s4.pathway1_res10.branch2.b_bn.weight
491 unfrozen s4.pathway1_res10.branch2.b_bn.bias
492 unfrozen s4.pathway1_res10.branch2.c.weight
493 unfrozen s4.pathway1_res10.branch2.c_bn.weight
494 unfrozen s4.pathway1_res10.branch2.c_bn.bias
495 unfrozen s4.pathway1_res11.branch2.a.weight
496 unfrozen s4.pathway1_res11.branch2.a_bn.weight
497 unfrozen s4.pathway1_res11.branch2.a_bn.bias
498 unfrozen s4.pathway1_res11.branch2.b.weight
499 unfrozen s4.pathway1_res11.branch2.b_bn.weight
500 unfrozen s4.pathway1_res11.branch2.b_bn.bias
501 unfrozen s4.pathway1_res11.branch2.c.weight
502 unfrozen s4.pathway1_res11.branch2.c_bn.weight
503 unfrozen s4.pathway1_res11.branch2.c_bn.bias
504 unfrozen s4.pathway1_res12.branch2.a.weight
505 unfrozen s4.pathway1_res12.branch2.a_bn.weight
506 unfrozen s4.pathway1_res12.branch2.a_bn.bias
507 unfrozen s4.pathway1_res12.branch2.b.weight
508 unfrozen s4.pathway1_res12.branch2.b_bn.weight
509 unfrozen s4.pathway1_res12.branch2.b_bn.bias
510 unfrozen s4.pathway1_res12.branch2.c.weight
511 unfrozen s4.pathway1_res12.branch2.c_bn.weight
512 unfrozen s4.pathway1_res12.branch2.c_bn.bias
513 unfrozen s4.pathway1_res13.branch2.a.weight
514 unfrozen s4.pathway1_res13.branch2.a_bn.weight
515 unfrozen s4.pathway1_res13.branch2.a_bn.bias
516 unfrozen s4.pathway1_res13.branch2.b.weight
517 unfrozen s4.pathway1_res13.branch2.b_bn.weight
518 unfrozen s4.pathway1_res13.branch2.b_bn.bias
519 unfrozen s4.pathway1_res13.branch2.c.weight
520 unfrozen s4.pathway1_res13.branch2.c_bn.weight
521 unfrozen s4.pathway1_res13.branch2.c_bn.bias
522 unfrozen s4.pathway1_res14.branch2.a.weight
523 unfrozen s4.pathway1_res14.branch2.a_bn.weight
524 unfrozen s4.pathway1_res14.branch2.a_bn.bias
525 unfrozen s4.pathway1_res14.branch2.b.weight
526 unfrozen s4.pathway1_res14.branch2.b_bn.weight
527 unfrozen s4.pathway1_res14.branch2.b_bn.bias
528 unfrozen s4.pathway1_res14.branch2.c.weight
529 unfrozen s4.pathway1_res14.branch2.c_bn.weight
530 unfrozen s4.pathway1_res14.branch2.c_bn.bias
531 unfrozen s4.pathway1_res15.branch2.a.weight
532 unfrozen s4.pathway1_res15.branch2.a_bn.weight
533 unfrozen s4.pathway1_res15.branch2.a_bn.bias
534 unfrozen s4.pathway1_res15.branch2.b.weight
535 unfrozen s4.pathway1_res15.branch2.b_bn.weight
536 unfrozen s4.pathway1_res15.branch2.b_bn.bias
537 unfrozen s4.pathway1_res15.branch2.c.weight
538 unfrozen s4.pathway1_res15.branch2.c_bn.weight
539 unfrozen s4.pathway1_res15.branch2.c_bn.bias
540 unfrozen s4.pathway1_res16.branch2.a.weight
541 unfrozen s4.pathway1_res16.branch2.a_bn.weight
542 unfrozen s4.pathway1_res16.branch2.a_bn.bias
543 unfrozen s4.pathway1_res16.branch2.b.weight
544 unfrozen s4.pathway1_res16.branch2.b_bn.weight
545 unfrozen s4.pathway1_res16.branch2.b_bn.bias
546 unfrozen s4.pathway1_res16.branch2.c.weight
547 unfrozen s4.pathway1_res16.branch2.c_bn.weight
548 unfrozen s4.pathway1_res16.branch2.c_bn.bias
549 unfrozen s4.pathway1_res17.branch2.a.weight
550 unfrozen s4.pathway1_res17.branch2.a_bn.weight
551 unfrozen s4.pathway1_res17.branch2.a_bn.bias
552 unfrozen s4.pathway1_res17.branch2.b.weight
553 unfrozen s4.pathway1_res17.branch2.b_bn.weight
554 unfrozen s4.pathway1_res17.branch2.b_bn.bias
555 unfrozen s4.pathway1_res17.branch2.c.weight
556 unfrozen s4.pathway1_res17.branch2.c_bn.weight
557 unfrozen s4.pathway1_res17.branch2.c_bn.bias
558 unfrozen s4.pathway1_res18.branch2.a.weight
559 unfrozen s4.pathway1_res18.branch2.a_bn.weight
560 unfrozen s4.pathway1_res18.branch2.a_bn.bias
561 unfrozen s4.pathway1_res18.branch2.b.weight
562 unfrozen s4.pathway1_res18.branch2.b_bn.weight
563 unfrozen s4.pathway1_res18.branch2.b_bn.bias
564 unfrozen s4.pathway1_res18.branch2.c.weight
565 unfrozen s4.pathway1_res18.branch2.c_bn.weight
566 unfrozen s4.pathway1_res18.branch2.c_bn.bias
567 unfrozen s4.pathway1_res19.branch2.a.weight
568 unfrozen s4.pathway1_res19.branch2.a_bn.weight
569 unfrozen s4.pathway1_res19.branch2.a_bn.bias
570 unfrozen s4.pathway1_res19.branch2.b.weight
571 unfrozen s4.pathway1_res19.branch2.b_bn.weight
572 unfrozen s4.pathway1_res19.branch2.b_bn.bias
573 unfrozen s4.pathway1_res19.branch2.c.weight
574 unfrozen s4.pathway1_res19.branch2.c_bn.weight
575 unfrozen s4.pathway1_res19.branch2.c_bn.bias
576 unfrozen s4.pathway1_res20.branch2.a.weight
577 unfrozen s4.pathway1_res20.branch2.a_bn.weight
578 unfrozen s4.pathway1_res20.branch2.a_bn.bias
579 unfrozen s4.pathway1_res20.branch2.b.weight
580 unfrozen s4.pathway1_res20.branch2.b_bn.weight
581 unfrozen s4.pathway1_res20.branch2.b_bn.bias
582 unfrozen s4.pathway1_res20.branch2.c.weight
583 unfrozen s4.pathway1_res20.branch2.c_bn.weight
584 unfrozen s4.pathway1_res20.branch2.c_bn.bias
585 unfrozen s4.pathway1_res21.branch2.a.weight
586 unfrozen s4.pathway1_res21.branch2.a_bn.weight
587 unfrozen s4.pathway1_res21.branch2.a_bn.bias
588 unfrozen s4.pathway1_res21.branch2.b.weight
589 unfrozen s4.pathway1_res21.branch2.b_bn.weight
590 unfrozen s4.pathway1_res21.branch2.b_bn.bias
591 unfrozen s4.pathway1_res21.branch2.c.weight
592 unfrozen s4.pathway1_res21.branch2.c_bn.weight
593 unfrozen s4.pathway1_res21.branch2.c_bn.bias
594 unfrozen s4.pathway1_res22.branch2.a.weight
595 unfrozen s4.pathway1_res22.branch2.a_bn.weight
596 unfrozen s4.pathway1_res22.branch2.a_bn.bias
597 unfrozen s4.pathway1_res22.branch2.b.weight
598 unfrozen s4.pathway1_res22.branch2.b_bn.weight
599 unfrozen s4.pathway1_res22.branch2.b_bn.bias
600 unfrozen s4.pathway1_res22.branch2.c.weight
601 unfrozen s4.pathway1_res22.branch2.c_bn.weight
602 unfrozen s4.pathway1_res22.branch2.c_bn.bias
603 unfrozen s4_fuse.conv_f2s.weight
604 unfrozen s4_fuse.bn.weight
605 unfrozen s4_fuse.bn.bias
606 unfrozen s5.pathway0_res0.branch1.weight
607 unfrozen s5.pathway0_res0.branch1_bn.weight
608 unfrozen s5.pathway0_res0.branch1_bn.bias
609 unfrozen s5.pathway0_res0.branch2.a.weight
610 unfrozen s5.pathway0_res0.branch2.a_bn.weight
611 unfrozen s5.pathway0_res0.branch2.a_bn.bias
612 unfrozen s5.pathway0_res0.branch2.b.weight
613 unfrozen s5.pathway0_res0.branch2.b_bn.weight
614 unfrozen s5.pathway0_res0.branch2.b_bn.bias
615 unfrozen s5.pathway0_res0.branch2.c.weight
616 unfrozen s5.pathway0_res0.branch2.c_bn.weight
617 unfrozen s5.pathway0_res0.branch2.c_bn.bias
618 unfrozen s5.pathway0_res1.branch2.a.weight
619 unfrozen s5.pathway0_res1.branch2.a_bn.weight
620 unfrozen s5.pathway0_res1.branch2.a_bn.bias
621 unfrozen s5.pathway0_res1.branch2.b.weight
622 unfrozen s5.pathway0_res1.branch2.b_bn.weight
623 unfrozen s5.pathway0_res1.branch2.b_bn.bias
624 unfrozen s5.pathway0_res1.branch2.c.weight
625 unfrozen s5.pathway0_res1.branch2.c_bn.weight
626 unfrozen s5.pathway0_res1.branch2.c_bn.bias
627 unfrozen s5.pathway0_res2.branch2.a.weight
628 unfrozen s5.pathway0_res2.branch2.a_bn.weight
629 unfrozen s5.pathway0_res2.branch2.a_bn.bias
630 unfrozen s5.pathway0_res2.branch2.b.weight
631 unfrozen s5.pathway0_res2.branch2.b_bn.weight
632 unfrozen s5.pathway0_res2.branch2.b_bn.bias
633 unfrozen s5.pathway0_res2.branch2.c.weight
634 unfrozen s5.pathway0_res2.branch2.c_bn.weight
635 unfrozen s5.pathway0_res2.branch2.c_bn.bias
636 unfrozen s5.pathway1_res0.branch1.weight
637 unfrozen s5.pathway1_res0.branch1_bn.weight
638 unfrozen s5.pathway1_res0.branch1_bn.bias
639 unfrozen s5.pathway1_res0.branch2.a.weight
640 unfrozen s5.pathway1_res0.branch2.a_bn.weight
641 unfrozen s5.pathway1_res0.branch2.a_bn.bias
642 unfrozen s5.pathway1_res0.branch2.b.weight
643 unfrozen s5.pathway1_res0.branch2.b_bn.weight
644 unfrozen s5.pathway1_res0.branch2.b_bn.bias
645 unfrozen s5.pathway1_res0.branch2.c.weight
646 unfrozen s5.pathway1_res0.branch2.c_bn.weight
647 unfrozen s5.pathway1_res0.branch2.c_bn.bias
648 unfrozen s5.pathway1_res1.branch2.a.weight
649 unfrozen s5.pathway1_res1.branch2.a_bn.weight
650 unfrozen s5.pathway1_res1.branch2.a_bn.bias
651 unfrozen s5.pathway1_res1.branch2.b.weight
652 unfrozen s5.pathway1_res1.branch2.b_bn.weight
653 unfrozen s5.pathway1_res1.branch2.b_bn.bias
654 unfrozen s5.pathway1_res1.branch2.c.weight
655 unfrozen s5.pathway1_res1.branch2.c_bn.weight
656 unfrozen s5.pathway1_res1.branch2.c_bn.bias
657 unfrozen s5.pathway1_res2.branch2.a.weight
658 unfrozen s5.pathway1_res2.branch2.a_bn.weight
659 unfrozen s5.pathway1_res2.branch2.a_bn.bias
660 unfrozen s5.pathway1_res2.branch2.b.weight
661 unfrozen s5.pathway1_res2.branch2.b_bn.weight
662 unfrozen s5.pathway1_res2.branch2.b_bn.bias
663 unfrozen s5.pathway1_res2.branch2.c.weight
664 unfrozen s5.pathway1_res2.branch2.c_bn.weight
665 unfrozen s5.pathway1_res2.branch2.c_bn.bias
666 unfrozen head.projection.weight
667 unfrozen head.projection.bias
[11/24 23:47:38][INFO] checkpoint.py: 507: Load from given checkpoint file.
[11/24 23:47:38][INFO] checkpoint.py: 214: Loading network weights from /srv/beegfs02/scratch/da_action/data/models_pretrained/SLOWFAST_32x2_R101_50_50.pkl.
[11/24 23:47:41][INFO] checkpoint.py: 340: Network weights head.projection.weight not loaded.
[11/24 23:47:41][INFO] checkpoint.py: 340: Network weights head.projection.bias not loaded.
[11/24 23:47:45][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/train.csv
[11/24 23:47:45][INFO] ava_helper.py: 111: Finished loading annotations from: /srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/ava_train_v2.2.csv
[11/24 23:47:45][INFO] ava_helper.py: 113: Detection threshold: 0.8
[11/24 23:47:45][INFO] ava_helper.py: 114: Number of unique boxes: 3847
[11/24 23:47:45][INFO] ava_helper.py: 115: Number of annotations: 5000
[11/24 23:47:45][INFO] ava_helper.py: 162: 2972 keyframes used.
[11/24 23:47:45][INFO] ava_dataset.py:  90: === AVA dataset summary ===
[11/24 23:47:45][INFO] ava_dataset.py:  91: Split: train
[11/24 23:47:45][INFO] ava_dataset.py:  92: Number of videos: 27
[11/24 23:47:45][INFO] ava_dataset.py:  96: Number of frames: 729814
[11/24 23:47:45][INFO] ava_dataset.py:  97: Number of key frames: 2972
[11/24 23:47:45][INFO] ava_dataset.py:  98: Number of boxes: 3847.
[11/24 23:47:46][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/val.csv
[11/24 23:47:46][INFO] ava_helper.py: 111: Finished loading annotations from: /srv/beegfs02/scratch/da_action/data/ava/annotations_10_500_100/ava_val_predicted_boxes.csv
[11/24 23:47:46][INFO] ava_helper.py: 113: Detection threshold: 0.8
[11/24 23:47:46][INFO] ava_helper.py: 114: Number of unique boxes: 1218
[11/24 23:47:46][INFO] ava_helper.py: 115: Number of annotations: 0
[11/24 23:47:46][INFO] ava_helper.py: 162: 694 keyframes used.
[11/24 23:47:46][INFO] ava_dataset.py:  90: === AVA dataset summary ===
[11/24 23:47:46][INFO] ava_dataset.py:  91: Split: val
[11/24 23:47:46][INFO] ava_dataset.py:  92: Number of videos: 6
[11/24 23:47:46][INFO] ava_dataset.py:  96: Number of frames: 162182
[11/24 23:47:46][INFO] ava_dataset.py:  97: Number of key frames: 694
[11/24 23:47:46][INFO] ava_dataset.py:  98: Number of boxes: 1218.
[11/24 23:47:49][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/train.csv
[11/24 23:47:49][INFO] ava_helper.py:  65: Finished loading image paths from: /srv/beegfs02/scratch/da_action/data/ava/frame_lists_10_500_100/val.csv
[11/24 23:47:49][INFO] tensorboard_vis.py:  57: To see logged results in Tensorboard, please launch using the command             `tensorboard  --port=<port-number> --logdir /srv/beegfs02/scratch/da_action/data/output/ex_10_500_100_v2/tensorboard`
[11/24 23:47:49][INFO] train_net.py: 464: Start epoch: 2
[11/25 00:25:51][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 00:25:51][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 00:25:51][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 00:25:51][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 00:25:52][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 00:25:52][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.10419458359461138,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08391267123287674,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.09376176399542921,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.16952535553910164,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.09640279394644935,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.1654999425226478,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.11659389159511044,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09204465358131397,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.12504395689152284,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.10415985063542263,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.11511394635344858}
[11/25 00:25:53][INFO] ava_eval_helper.py: 174: AVA eval done in 1.247904 seconds.
[11/25 00:25:53][INFO] logging.py:  97: json_stats: {
  "RAM": "19.16/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "2",
  "gpu_mem": "4.48G",
  "map": 0.11511,
  "mode": "val"
}
[11/25 01:07:07][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 01:07:07][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 01:07:07][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 01:07:07][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 01:07:07][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 01:07:07][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.1744506263130748,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.12590221889898517,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.12041653978621547,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.09555393949677518,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.17165770458033677,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.09371584541276534,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.09850882748137649,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.11373183838773726,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.0889605263063884,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.07585185185185184,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.11587499185155067}
[11/25 01:07:08][INFO] ava_eval_helper.py: 174: AVA eval done in 1.345951 seconds.
[11/25 01:07:08][INFO] logging.py:  97: json_stats: {
  "RAM": "20.20/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "3",
  "gpu_mem": "4.48G",
  "map": 0.11587,
  "mode": "val"
}
[11/25 01:47:15][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 01:47:15][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 01:47:15][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 01:47:15][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 01:47:15][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 01:47:15][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.1378283056283391,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.0806003289473684,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.08472911635313807,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1039029346310754,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.07722651782461912,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.13417945020491331,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.09546375332744328,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10761749550853589,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.13876524303315188,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.07566502463054187,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.10359781700891264}
[11/25 01:47:16][INFO] ava_eval_helper.py: 174: AVA eval done in 1.317550 seconds.
[11/25 01:47:16][INFO] logging.py:  97: json_stats: {
  "RAM": "19.95/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "4",
  "gpu_mem": "4.48G",
  "map": 0.10360,
  "mode": "val"
}
[11/25 02:27:31][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 02:27:31][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 02:27:31][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 02:27:31][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 02:27:31][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 02:27:31][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.16143225181169393,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.0806003289473684,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.08139243736610269,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.14015126289769902,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.19816706106936469,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.18866971238613717,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.15474704542461656,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10773971032221201,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.14577939135105833,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08438122431578823,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.1343060425892041}
[11/25 02:27:32][INFO] ava_eval_helper.py: 174: AVA eval done in 1.355271 seconds.
[11/25 02:27:32][INFO] logging.py:  97: json_stats: {
  "RAM": "18.78/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "5",
  "gpu_mem": "4.48G",
  "map": 0.13431,
  "mode": "val"
}
[11/25 03:08:19][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 03:08:19][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 03:08:19][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 03:08:19][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 03:08:19][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 03:08:19][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.13911219447063142,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08630979929017533,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.10989813307184002,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1728848472728874,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.5030190219375845,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.2264291880243319,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.1180964694957117,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09552848992084817,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.1442620468117241,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.0844768880878954,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.168001707838363}
[11/25 03:08:20][INFO] ava_eval_helper.py: 174: AVA eval done in 1.308547 seconds.
[11/25 03:08:20][INFO] logging.py:  97: json_stats: {
  "RAM": "19.86/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "6",
  "gpu_mem": "4.48G",
  "map": 0.16800,
  "mode": "val"
}
[11/25 03:49:25][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 03:49:25][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 03:49:25][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 03:49:25][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 03:49:25][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 03:49:25][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.1663237245985315,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08236571763591312,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.1572077120169808,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.150016266075862,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.3011122012022584,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.2284867047397971,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.12100559107181916,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10180176784670378,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.18276911107263344,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08141342756183745,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.15725022238223368}
[11/25 03:49:26][INFO] ava_eval_helper.py: 174: AVA eval done in 1.247711 seconds.
[11/25 03:49:26][INFO] logging.py:  97: json_stats: {
  "RAM": "20.08/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "7",
  "gpu_mem": "4.48G",
  "map": 0.15725,
  "mode": "val"
}
[11/25 04:31:33][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 04:31:33][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 04:31:33][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 04:31:33][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 04:31:33][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 04:31:33][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.1657980133414233,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.10121180675660098,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.19998462287544613,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.17785768721671466,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.06798850574712643,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.2203241589855256,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.14988151178032888,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10212153487323322,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.23394403360808103,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.09887956679658974,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.15179914419810697}
[11/25 04:31:34][INFO] ava_eval_helper.py: 174: AVA eval done in 1.357748 seconds.
[11/25 04:31:34][INFO] logging.py:  97: json_stats: {
  "RAM": "19.28/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "8",
  "gpu_mem": "4.48G",
  "map": 0.15180,
  "mode": "val"
}
[11/25 05:12:33][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 05:12:33][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 05:12:33][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 05:12:33][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 05:12:33][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 05:12:33][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.2051049657954117,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08612478031634446,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.16860270501296426,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.11311404325192248,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.10687120542292956,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.21661005757348636,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.1574309584770517,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10760939369686194,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.13711029216223325,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08708299671914671,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.13856613984283525}
[11/25 05:12:34][INFO] ava_eval_helper.py: 174: AVA eval done in 1.358226 seconds.
[11/25 05:12:34][INFO] logging.py:  97: json_stats: {
  "RAM": "19.73/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "9",
  "gpu_mem": "4.48G",
  "map": 0.13857,
  "mode": "val"
}
[11/25 05:54:10][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 05:54:10][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 05:54:10][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 05:54:10][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 05:54:10][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 05:54:10][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.17587613024477852,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08693379998510688,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.20818876904748307,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1357215439213085,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.3445973449637426,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.26292465241770596,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.15860909901653947,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.12141291656630597,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.12854208324395627,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08101012173118285,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.170381646113811}
[11/25 05:54:11][INFO] ava_eval_helper.py: 174: AVA eval done in 1.436293 seconds.
[11/25 05:54:11][INFO] logging.py:  97: json_stats: {
  "RAM": "19.22/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "10",
  "gpu_mem": "4.48G",
  "map": 0.17038,
  "mode": "val"
}
[11/25 06:37:59][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 06:37:59][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 06:37:59][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 06:37:59][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 06:37:59][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 06:37:59][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.13759403204322726,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.0974835926554089,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.19882898477556524,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1501145824837964,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.06798850574712643,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.22846147051973625,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.12349848438246464,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09147273862795487,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.1920131393237332,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.09137951575285783,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.1378835046311871}
[11/25 06:38:00][INFO] ava_eval_helper.py: 174: AVA eval done in 1.309290 seconds.
[11/25 06:38:00][INFO] logging.py:  97: json_stats: {
  "RAM": "19.58/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "11",
  "gpu_mem": "4.48G",
  "map": 0.13788,
  "mode": "val"
}
[11/25 07:22:26][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 07:22:26][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 07:22:26][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 07:22:26][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 07:22:26][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 07:22:26][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.16659339948564073,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08126865671641792,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.1444269837458425,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.13820406786339962,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.06804437140509449,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.18452610713672118,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.10662040634759087,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.08588597777042929,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.12017160718802924,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.07908282923343449,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.11748244068926003}
[11/25 07:22:28][INFO] ava_eval_helper.py: 174: AVA eval done in 1.282385 seconds.
[11/25 07:22:28][INFO] logging.py:  97: json_stats: {
  "RAM": "20.72/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "12",
  "gpu_mem": "4.48G",
  "map": 0.11748,
  "mode": "val"
}
[11/25 08:07:08][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 08:07:08][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 08:07:08][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 08:07:08][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 08:07:08][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 08:07:08][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.150152126716553,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08416176521059358,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.17458253511935629,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.16468522079041698,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.34358602209143096,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.2534877290004222,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.12883658456646718,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.0903972718356006,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.15355616865204996,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08187918573184111,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.1625324609714732}
[11/25 08:07:09][INFO] ava_eval_helper.py: 174: AVA eval done in 1.432621 seconds.
[11/25 08:07:09][INFO] logging.py:  97: json_stats: {
  "RAM": "19.36/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "13",
  "gpu_mem": "4.48G",
  "map": 0.16253,
  "mode": "val"
}
[11/25 08:52:09][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 08:52:09][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 08:52:09][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 08:52:09][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 08:52:09][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 08:52:09][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.11899856869101698,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08506611947104425,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.11370401630301757,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.149169754815472,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.12450681146542751,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4021586712902876,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.19468273506097786,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09987891061340151,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.11703712700737977,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08338861497086511,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.14885913296888903}
[11/25 08:52:10][INFO] ava_eval_helper.py: 174: AVA eval done in 1.287805 seconds.
[11/25 08:52:10][INFO] logging.py:  97: json_stats: {
  "RAM": "19.70/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "14",
  "gpu_mem": "4.48G",
  "map": 0.14886,
  "mode": "val"
}
[11/25 09:36:00][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 09:36:00][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 09:36:00][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 09:36:00][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 09:36:00][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 09:36:00][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.21395163507121212,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08367007672634272,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.18081115473512901,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.15390705398688603,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.09733533839786074,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.15266697280233402,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.13250628249105625,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09896473952753584,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.17061028614404183,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08243119491840872,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.13668547348008075}
[11/25 09:36:01][INFO] ava_eval_helper.py: 174: AVA eval done in 1.289686 seconds.
[11/25 09:36:01][INFO] logging.py:  97: json_stats: {
  "RAM": "17.29/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "15",
  "gpu_mem": "4.48G",
  "map": 0.13669,
  "mode": "val"
}
[11/25 10:21:54][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 10:21:54][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 10:21:54][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 10:21:54][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 10:21:54][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 10:21:54][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.14221169827130511,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08405660377358491,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.17123421577880188,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.18506927021091774,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.110548565190573,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.2261271665166893,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.14928055247401936,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10530529849770157,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.16715936312416743,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08242617624505144,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.14234189100828118}
[11/25 10:21:55][INFO] ava_eval_helper.py: 174: AVA eval done in 1.390975 seconds.
[11/25 10:21:55][INFO] logging.py:  97: json_stats: {
  "RAM": "17.77/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "16",
  "gpu_mem": "4.48G",
  "map": 0.14234,
  "mode": "val"
}
[11/25 11:11:39][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 11:11:39][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 11:11:39][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 11:11:39][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 11:11:39][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 11:11:39][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.15938002532988027,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08066666666666665,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.1439291265263811,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1692539543541035,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.22487357921889103,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.1626973421582169,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.12141060712093764,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.0963573337434121,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.16855647803294543,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.07818184455509893,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.14053069577065336}
[11/25 11:11:40][INFO] ava_eval_helper.py: 174: AVA eval done in 1.328866 seconds.
[11/25 11:11:40][INFO] logging.py:  97: json_stats: {
  "RAM": "17.82/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "17",
  "gpu_mem": "4.48G",
  "map": 0.14053,
  "mode": "val"
}
[11/25 12:07:18][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 12:07:18][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 12:07:18][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 12:07:18][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 12:07:18][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 12:07:18][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.13035628709731795,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08099999999999997,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.1272649325112031,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1805419771072972,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.1352332642003511,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.22800289117681918,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.11302750832268002,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.08543931333205884,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.1444620916404408,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.07874195378943502,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.13040702191776032}
[11/25 12:07:20][INFO] ava_eval_helper.py: 174: AVA eval done in 1.272521 seconds.
[11/25 12:07:20][INFO] logging.py:  97: json_stats: {
  "RAM": "13.21/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "18",
  "gpu_mem": "4.48G",
  "map": 0.13041,
  "mode": "val"
}
[11/25 13:03:50][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 13:03:50][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 13:03:50][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 13:03:50][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 13:03:50][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 13:03:50][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.14233065300997963,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08929019870799368,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.15813473147127266,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1697510149765898,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.07685355862775217,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.199527953663604,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.15064816121851005,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09087746622884493,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.14763452629480725,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08202795209521224,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.13070762162945665}
[11/25 13:03:51][INFO] ava_eval_helper.py: 174: AVA eval done in 1.399125 seconds.
[11/25 13:03:51][INFO] logging.py:  97: json_stats: {
  "RAM": "13.05/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "19",
  "gpu_mem": "4.48G",
  "map": 0.13071,
  "mode": "val"
}
[11/25 13:58:24][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 13:58:24][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 13:58:24][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 13:58:24][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 13:58:24][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 13:58:24][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.1718466160757205,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.0829226441631505,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.17860546350496964,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1646032833721922,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.16208946697720922,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.19013626116575358,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.13838112883420117,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09006710909546978,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.15973105229379356,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.07951998080353039,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.14179030062859907}
[11/25 13:58:25][INFO] ava_eval_helper.py: 174: AVA eval done in 1.323218 seconds.
[11/25 13:58:25][INFO] logging.py:  97: json_stats: {
  "RAM": "13.08/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "20",
  "gpu_mem": "4.48G",
  "map": 0.14179,
  "mode": "val"
}
[11/25 14:50:21][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 14:50:21][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 14:50:21][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 14:50:21][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 14:50:21][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 14:50:21][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.17609007343957755,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08046798029556651,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.16298328800327982,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.17476947005544033,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.09837380577880249,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.14733415999453764,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.1941888227763802,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09407634405897415,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.1272467303990521,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.0781139852425845,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.1333644660044195}
[11/25 14:50:22][INFO] ava_eval_helper.py: 174: AVA eval done in 1.302881 seconds.
[11/25 14:50:22][INFO] logging.py:  97: json_stats: {
  "RAM": "13.01/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "21",
  "gpu_mem": "4.48G",
  "map": 0.13336,
  "mode": "val"
}
[11/25 15:43:40][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 15:43:40][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 15:43:40][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 15:43:40][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 15:43:40][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 15:43:40][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.1440653337944194,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08126865671641792,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.140793617250103,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.15061212032727161,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.1332824519871266,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.1269801264964911,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.1400311420921777,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10397801243924203,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.1257599463311223,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.07800007957506913,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.12247714870094409}
[11/25 15:43:41][INFO] ava_eval_helper.py: 174: AVA eval done in 1.388560 seconds.
[11/25 15:43:41][INFO] logging.py:  97: json_stats: {
  "RAM": "12.57/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "22",
  "gpu_mem": "4.48G",
  "map": 0.12248,
  "mode": "val"
}
[11/25 16:42:27][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 16:42:27][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 16:42:27][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 16:42:27][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 16:42:27][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 16:42:27][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.13341042214425786,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08727515583259127,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.14076049161798618,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.17672158785464553,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.12191234698727742,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.15386038176810043,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.22987392692852915,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09956055954955927,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.13067850750989024,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08667195585683925,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.13607253360496765}
[11/25 16:42:28][INFO] ava_eval_helper.py: 174: AVA eval done in 1.336643 seconds.
[11/25 16:42:28][INFO] logging.py:  97: json_stats: {
  "RAM": "9.89/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "23",
  "gpu_mem": "4.48G",
  "map": 0.13607,
  "mode": "val"
}
[11/25 17:38:56][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 17:38:56][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 17:38:56][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 17:38:56][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 17:38:56][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 17:38:56][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.16942735056313252,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08099999999999997,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.15446007553809027,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.17923313644658648,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.06986121119925222,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.13879049770541765,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.17901090142778786,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.08953583811521937,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.12882108348059979,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08046632061739731,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.12706064150934834}
[11/25 17:38:57][INFO] ava_eval_helper.py: 174: AVA eval done in 1.327008 seconds.
[11/25 17:38:57][INFO] logging.py:  97: json_stats: {
  "RAM": "12.52/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "24",
  "gpu_mem": "4.48G",
  "map": 0.12706,
  "mode": "val"
}
[11/25 18:34:53][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 18:34:53][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 18:34:53][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 18:34:53][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 18:34:53][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 18:34:53][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.13899315365252052,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08673451327433626,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.17103938951726272,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.19665757996142186,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.10503252293891932,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.16744274677520685,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.18648913074533777,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09558917111707557,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.12921778992579255,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08508370862588524,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.13622797065337586}
[11/25 18:34:54][INFO] ava_eval_helper.py: 174: AVA eval done in 1.430523 seconds.
[11/25 18:34:54][INFO] logging.py:  97: json_stats: {
  "RAM": "12.81/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "25",
  "gpu_mem": "4.48G",
  "map": 0.13623,
  "mode": "val"
}
[11/25 19:31:32][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 19:31:32][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 19:31:32][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 19:31:32][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 19:31:32][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 19:31:32][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.11667687575581653,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08298899237933953,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.15841167880885332,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.1934939655577959,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.08827545268100156,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.15181168191758634,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.17538968957520734,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.10808307561233609,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.13298220706282002,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.0799848581735019,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.12880984775242585}
[11/25 19:31:33][INFO] ava_eval_helper.py: 174: AVA eval done in 1.318134 seconds.
[11/25 19:31:33][INFO] logging.py:  97: json_stats: {
  "RAM": "12.49/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "26",
  "gpu_mem": "4.48G",
  "map": 0.12881,
  "mode": "val"
}
[11/25 20:27:49][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 20:27:49][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 20:27:49][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 20:27:49][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 20:27:49][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 20:27:49][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.18714636254721295,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08257142857142859,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.1329170259501557,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.14343862264490032,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.11273426970708293,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.12005433128136278,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.17451139545927602,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.09642614525444698,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.14767487548214936,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08127588098176333,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.1278750337879779}
[11/25 20:27:50][INFO] ava_eval_helper.py: 174: AVA eval done in 1.376884 seconds.
[11/25 20:27:50][INFO] logging.py:  97: json_stats: {
  "RAM": "12.99/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "27",
  "gpu_mem": "4.48G",
  "map": 0.12788,
  "mode": "val"
}
[11/25 21:19:40][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 21:19:40][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 21:19:40][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 21:19:40][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 21:19:40][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 21:19:40][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.11903947944901802,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08758713136729222,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.1556905059073719,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.13841957694528367,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.11075591889971936,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.2094770843796959,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.23002843250072558,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.0882440785671673,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.12358356508647061,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08529980992467764,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.13481255830274222}
[11/25 21:19:41][INFO] ava_eval_helper.py: 174: AVA eval done in 1.372550 seconds.
[11/25 21:19:41][INFO] logging.py:  97: json_stats: {
  "RAM": "13.00/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "28",
  "gpu_mem": "4.48G",
  "map": 0.13481,
  "mode": "val"
}
[11/25 22:04:56][INFO] ava_eval_helper.py: 164: Evaluating with 700 unique GT frames.
[11/25 22:04:56][INFO] ava_eval_helper.py: 166: Evaluating with 694 unique detection frames
[11/25 22:04:56][INFO] ava_eval_helper.py: 314: AVA results wrote to detections_latest.csv
[11/25 22:04:56][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
[11/25 22:04:56][INFO] ava_eval_helper.py: 314: AVA results wrote to groundtruth_latest.csv
[11/25 22:04:56][INFO] ava_eval_helper.py: 315: 	took 0 seconds.
{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.12445376137183592,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.08507812499999998,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': 0.12657331610163702,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.12128718733422232,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': 0.11361936357275473,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.11076057788768168,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.16881911527845334,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.0908131128548103,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.13400457295900137,
  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.08246060634088714,
  'PascalBoxes_Precision/mAP@0.5IOU': 0.11578697387012837}
[11/25 22:04:57][INFO] ava_eval_helper.py: 174: AVA eval done in 1.317845 seconds.
[11/25 22:04:57][INFO] logging.py:  97: json_stats: {
  "RAM": "13.16/251.81G",
  "_type": "val_epoch",
  "cur_epoch": "29",
  "gpu_mem": "4.48G",
  "map": 0.11579,
  "mode": "val"
}
slurmstepd: error: *** JOB 172902 ON biwirender07 CANCELLED AT 2020-11-25T22:46:50 ***
